[ { "title": "Centos8 Development Of Machine", "url": "/posts/centos8-development-of-machine/", "categories": "Linux, CentOS", "tags": "", "date": "2022-06-05 00:00:00 +0800", "snippet": "本人CentOS开发服务器换了块硬盘，因此需要重新安装一下，为了下次加快安装设置速度，把遇到的问题记录一下CentOS8初始安装介质是CentOS 8.2，从光盘或者U盘启动，进入安装程序： 语言：English 时区：Asia/Shanghai 根据需要选择安装软件 安装磁盘：sda1，如果空间不够选择清空磁盘 网络开关打开以太网 设置root密码（不设置无法远程登录） 设置一个新用户 “admin” 安装完成，重启系统以root用户登录，完成一些初始配置network第一步设置网络，否则没有连接# 下面命令中的enp2s0视不同主机的网卡名称会有所不同，按照实际替换即可vi /etc/sysconfig/network-scripts/ifcfg-enp2s0配置文件的初始值ONBOOT=no，意味着不会随系统启动而启动，所以开机后网络处于不可用状态，必须改掉使用DHCP自动分配IP的配置如下TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=noIPV6_AUTOCONF=noIPV6_DEFROUTE=noIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp2s0UUID=1184bd2c-04f9-4169-8a4d-f9191cf60542DEVICE=enp2s0ONBOOT=yes使用static静态IP配置如下TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=noIPV6_AUTOCONF=noIPV6_DEFROUTE=noIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp2s0UUID=1184bd2c-04f9-4169-8a4d-f9191cf60542DEVICE=enp2s0IPADDR=192.168.0.100NETMASK=255.255.255.0GATEWAY=192.168.0.1ONBOOT=yes修改完成后重启网卡# 重新加载配置文件nmcli c reload# 重启网卡nmcli c up enp2s0更换yum源为国内阿里源由于Centos8的官方源已更换，我安装好的系统根本访问不了默认yum源，直接全部删除重配置sudo rm -f /etc/yum.repos.d/*.reposudo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo打开下载的CentOS-Base.repo添加如下内容[epel]name=epelbaseurl=https://mirrors.aliyun.com/epel/8/Everything/x86_64/gpgcheck=1gpgkey=https://mirrors.aliyun.com/epel/RPM-GPG-KEY-EPEL-8更新yum源yum -y update" }, { "title": "修改bash窗口前导提示", "url": "/posts/bash-pre-symbol/", "categories": "Linux", "tags": "", "date": "2022-05-29 11:01:00 +0800", "snippet": "上次修改桌面布局后，terminal 窗口宽度变得较为固定，此时遇到一个问题是当我进入一个名称较长的目录后窗口的前导提示已经占据了一行中大半的空间，体验非常不好，于是决定动手修改一下。先查看一下当前所使用的前导格式echo $PS1&amp;gt; \\h:\\W \\u\\$这里贴一下格式说明： 变量 描述 \\d 当前系统日期 \\t 系统时间 \\h 主机名 # 命令符号 \\u 用户名 \\W 当前路径名 \\w 当前完整路径 修改为只显示当前路径名的格式，因为要修改系统级配置，需要加上sudo使用root权限sudo sed -i &#39;&#39; &#39;s/\\\\h:\\\\W \\\\u/\\\\W/g&#39; /etc/bashrc# 检查修改是否正确cat /etc/bash | grep PS1=&amp;gt; PS1=&#39;\\W\\$&#39;# 使修改生效source /etc/bashrc" }, { "title": "Flink CDC 2.2", "url": "/posts/flink-cdc-2.2/", "categories": "Big data, Flink", "tags": "", "date": "2022-05-28 00:00:00 +0800", "snippet": "Flink CDC 发布了2.2版本，这次新增加了4种数据源：OceanBase，PolarDB-X，SqlServer，TiDB；这其中的三种：OceanBase，SqlServer，TiDB应该算是比较常见的数据库了，而且对于MySQL的同步支持动态加表，也就是说可以在原来的同步不中断的情况下增加对新表的同步，可以说是一个非常重要的功能。 to be continue…" }, { "title": "Linux Usb Image", "url": "/posts/linux-usb-image/", "categories": "Linux", "tags": "", "date": "2020-12-25 00:00:00 +0800", "snippet": "做物联网相关项目时，经常需要在众多同类主机上安装Linux系统，并配置上一系列软件，手动一台台安装又慢又容易出错，此时可采用USB设备安装方案，具体做法是使用量产已安装好Linux系统及软件的U盘，怼到主机上就可以运行了，先按照需要制作好Linux镜像，然后将镜像写入USB盘，最后将USB盘插上主机USB口设置为USB引导开机。Linux发行版选择选定Linux发行版，在主机上进行系统安装和软件功能测试，确保系统功能兼容和软件可用性，完成测试后就可以选定发行版了。下面以ArchLinux发行版为例编写调度安装脚本 在开发机上安装VMWare或Virtualbox虚拟机软件 下载ArchLinux发行版iso镜像文件 创建一个Linux虚拟机并设置为iso镜像启动 准备1个8G以上U盘（或者USB移动硬盘）空盘，容量大小视要安装的系统及软件大小而定，16G以上容量更为稳妥 将U盘接入宿主机，并设置虚拟机对USB设备的访问，VMWare 或 Virtualbox有不同的设置方式 启动虚拟机从光驱镜像进入ArchLinux系统 编写在USB设备上安装Linux系统的初始化脚本#!/usr/bin/bash#1.Network Testtimedatectl set-ntp truedhcpcdecho &quot;ArchLinux to USB Flash Disk Auto Install&quot;echo &quot;Ver:1.0&quot;echo &quot;github.com/multichian/arch-usb-installer&quot;#2.Disk Partitionlsblkread -t 5 -p &quot;Select Your USB Disk (Default &#39;sdb&#39;):&quot; DISKDISK=${DISK:-&#39;sdb&#39;}read -t 5 -p &quot;Storage Size(Default 200M):&quot; STORAGESTORAGE=${STORAGE:-200}read -t 5 -p &quot;EFI Size(Default 100M):&quot; EFIEFI=${EFI:-100}OS=`expr 8000 - $STORAGE - $EFI`echo &quot;OS Size:&quot; $OSread -t 5 -p &quot;Are you sure you choice is correct? Enter N to restart: &quot; AXif [[ ${AX} = N ]]; then exitfifdisk /dev/${DISK} &amp;lt;&amp;lt;EOF d o n p +${EFI}M n p +${STORAGE}M n p +${OS}M wqEOFecho &quot;Partition USB Disk Done&quot;#3.Wipe And Mount Diskmkfs.vfat /dev/${DISK}1 -n EFImkfs.vfat /dev/${DISK}2 -n MCOSmkfs.ext4 -O &quot;^has_journal&quot; /dev/${DISK}3mount /dev/${DISK}3 /mntmkdir -p /mnt/boot/efimount /dev/${DISK}1 /mnt/boot/efiecho &quot;Wipe And Mount USB Disk Done&quot;#4.Using China Mirrorlistmv /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.old#echo -e &quot;https://mirrors.ustc.edu.cn/archlinux/$repo/os/$arch\\n&quot;echo &quot;Server=https://mirrors.ustc.edu.cn/archlinux/\\$repo/os/\\$arch&quot; &amp;gt;&amp;gt; /etc/pacman.d/mirrorlistecho &quot;Server=https://mirrors.aliyun.com/archlinux/\\$repo/os/\\$arch&quot; &amp;gt;&amp;gt; /etc/pacman.d/mirrorlist#echo -e &quot;\\n## China mirrors\\nhttps://mirrors.ustc.edu.cn/archlinux/\\$repo/os/\\$arch\\nhttps://mirrors.163.com/archlinux/\\$repo/os/\\$arch&quot; &amp;gt;&amp;gt; mirrorlistecho &quot;Change China Source Done&quot;#5.Install System#pacstrap /mnt base base-devel linux linux-firmware dhcpcd ntfs-3g dialog vim wireless_tools wpa_supplicant net-tools dosfstools opensshpacstrap /mnt base base-devel linux linux-firmware dhcpcd vim net-tools sudo grub efibootmgrecho &quot;Install Package Done&quot;#6.FSTABgenfstab -U -p /mnt &amp;gt;&amp;gt; /mnt/etc/fstabcat /mnt/etc/fstab#7.Enter New Systemarch-chroot /mnt执行完成该脚本后，USB设备上已经安装好Arch Linux基本系统，下一步需要做一些引导配置、用户密码及自动登录设置，因为这些需要在已经安装好的系统上执行，因此需要再编写一个脚本#8.Locale and Langecho -e &quot;en_US.UTF-8 UTF-8\\nzh_CN.UTF-8 UTF-8\\n&quot; &amp;gt; /etc/locale.genlocale-genecho -e &quot;LANG=en_US.UTF-8\\nLC_TYPE=en_US.UTF-8&quot; &amp;gt; /etc/locale.confln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimehwclock --systohc --localtime#9.Machine infoecho &quot;MCOS&quot; &amp;gt; /etc/hostnameecho -e &#39;127.0.0.1 localhost\\n::1 localhost\\n127.0.1.1 mcos.localdomain mcos&#39; &amp;gt;&amp;gt; /etc/hostssed -i &#39;/^HOOKS/cHOOKS=(base udev block keyboard autodetect modconf filesystems fsck)&#39; /etc/mkinitcpio.confmkinitcpio -Psystemctl enable dhcpcdsystemctl enable sshdecho &quot;Create New initramfs Done&quot;#10.Install GRUBecho &quot;Install Grub&quot;lsblkread -p &quot;Select Your USB Disk:&quot; DISKgrub-install --target=i386-pc /dev/${DISK}grub-install --target=x86_64-efi --efi-directory=/boot/efi --removable --recheckgrub-mkconfig -o /boot/grub/grub.cfgcp -v /usr/share/grub/{unicode.pf2,ascii.pf2} /boot/grub/cp -v /usr/share/locale/en\\@quot/LC_MESSAGES/grub.mo /boot/grub/locale/en.moecho &quot;Install Grub Done&quot;#11.Setup Autologinecho &quot;Autologin&quot;mkdir /etc/systemd/system/getty@tty1.service.d/cd /etc/systemd/system/getty@tty1.service.d/echo -e &quot;[Service]\\nExecStart=\\nExecStart=-/usr/bin/agetty --autologin miner --noclear %I 38400 linux&quot; &amp;gt; autologin.conf#12. Initialize User and passwordecho &quot;Initialize the Users&quot;echo &quot;Set Root passwd:&quot;passwduseradd -m -G wheel,audio,video,optical,storage minerecho &quot;Set miner passwd:&quot;passwd minerread -p &quot;Do you want VISUDO? Enter Y or N&quot; VXif [[ ${VX} = Y ]]; then visudofi## %wheel ALL=(ALL) ALL## $wheel ALL=(ALL) NOPASSWD: ALLecho &quot;User initialization is complete&quot;read -p &quot;Do you want to poweroff? Enter Y to shutdown:&quot; CXif [[ ${CX} = Y ]]; then poweroffelse exitfi到这一步可以USB启动系统并登录了，还有一此基于用户的配置和软件安装，这一步可以自主定制，比如安装业务软件什么的…#13. Install Openbox# 需要在登录用户下操作sudo pacman -Sy xorg-server xorg-xinit xterm openbox # 安装字体sudo pacman -S noto-fonts-cjk python python-pip# 安装扩展字体sudo pacman -S ttf-dejavu ttf-liberation wqy-zenhei ttf-arphic-ukai ttf-arphic-uming mkdir -p ~/.config/openboxcp /etc/xdg/openbox/* ~/.config/openbox/echo &quot;xterm -hold -e cal&quot; &amp;gt;&amp;gt; ~/.config/openbox/autostartecho &quot;exec openbox-session&quot; &amp;gt; ~/.xinitrcecho &quot;#Auto startx&quot;echo &quot;if [ -z \\&quot;\\$DISPLAY\\&quot; ] &amp;amp;&amp;amp; [ -n \\&quot;\\$XDG_VTNR\\&quot; ] &amp;amp;&amp;amp; [ \\&quot;\\$XDG_VTNR\\&quot; -eq 1 ]; then&quot;echo &quot; exec startx&quot;echo &quot;fi&quot;#14. Install environmentsudo pacman -S python python-pippip install psutil GPUtil tabulate一切安装完毕后，将USB盘内容制成img磁盘镜像，Mac/Linux系统中可以用dd命令或者安装镜像制作软件dd if=/dev/sdb1 of=./archlinux.img bs=10m然后换上其它空U盘均可以用这个img文件写入，因为U盘写入速度较慢，可以几个USB口插满U盘然后多条命令并行写入，大大缩短制作时间到这一步，可能有人会说：“也可以不编写脚本啊，我只要在虚拟机里把系统制作好，再制成镜像就行。” 没错！制作脚本的目的主要是为了方便维护和分发，从脚本就可以很清楚知道在系统基础上做了哪些设置和封装，后续要更改集成的软件，或者升级之类只需要修改脚本就行，不需要一直虚拟机镜像和img文件，分发文件体积会大大缩小" }, { "title": "Canal Server Config", "url": "/posts/canal-server-config/", "categories": "", "tags": "", "date": "2019-10-11 00:00:00 +0800", "snippet": "canal是一款MySQL增量日志解析并提供订阅&amp;amp;消费消息中间件，通过配置canal伪装为MySQL的Slave节点，接收解析MySQL的同步日志将其转化为相应消息系统消息，原始使用场景是做为otter中间件的一部分，后来成为独立开源项目，已经贡献给Apache基金会。canal常见使用场景有4种，除做为otter组件外，还可以做为 缓存同步触发消息 自动填充数据库拉链表 做为实时统计数据源canal在大数据场景较为常用（上面第4个场景），通过订阅业务MySQL库的数据变更并转化为kafka消息，再由Flink计算程序进行实时计算，可以较好满足业务方实时数仓建设需求。拉取所需docker镜像这次在docker容器部署一个canal server做为开发环境docker pull canal/canal-serverdocker pull mysql_master:5.7.1在MySQL中配置canal用户docker exec -it mysql_master bashCREATE USER canal IDENTIFIED BY &#39;canal&#39;;GRANT SELECT, SHOW VIEW, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &#39;canal&#39;@&#39;%&#39;;FLUSH PRIVILEGES;启动canal serverdocker run --name canal-server \\ -e canal.instance.master.address=10.50.9.55:3406 \\ -e canal.instance.dbUsername=canal \\ -e canal.instance.dbPassword=canal \\ -p 11111:11111 \\ -d canal/canal-server:v1.1.4./run_server.sh -e canal.admin.manager=10.50.9.55:8089 \\ -e canal.admin.port=11110 \\ -e canal.admin.user=admin \\ -e canal.admin.passwd=******canal.properties也可以把配置写到配置文件中，然后启动时指定配置文件启动########################################################## common argument ############################################################### tcp bind ipcanal.ip =# register ip to zookeepercanal.register.ip =canal.port = 11111canal.metrics.pull.port = 11112# canal instance user/passwdcanal.user = canalcanal.passwd = E3619321C1A937C46A0D8BD1DAC39F93B27D4458# canal admin configcanal.admin.manager = 127.0.0.1:8089canal.admin.port = 11110canal.admin.user = admincanal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441canal.zkServers =# flush data to zkcanal.zookeeper.flush.period = 1000canal.withoutNetty = false# tcp, kafka, RocketMQcanal.serverMode = tcp# flush meta cursor/parse position to filecanal.file.data.dir = ${canal.conf.dir}canal.file.flush.period = 1000## memory store RingBuffer size, should be Math.pow(2,n)canal.instance.memory.buffer.size = 16384## memory store RingBuffer used memory unit size , default 1kbcanal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZEcanal.instance.memory.batch.mode = MEMSIZEcanal.instance.memory.rawEntry = true## detecing configcanal.instance.detecting.enable = false#canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now()canal.instance.detecting.sql = select 1canal.instance.detecting.interval.time = 3canal.instance.detecting.retry.threshold = 3canal.instance.detecting.heartbeatHaEnable = false# support maximum transaction size, more than the size of the transaction will be cut into multiple transactions deliverycanal.instance.transaction.size = 1024# mysql fallback connected to new master should fallback timescanal.instance.fallbackIntervalInSeconds = 60# network configcanal.instance.network.receiveBufferSize = 16384canal.instance.network.sendBufferSize = 16384canal.instance.network.soTimeout = 30# binlog filter configcanal.instance.filter.druid.ddl = truecanal.instance.filter.query.dcl = falsecanal.instance.filter.query.dml = falsecanal.instance.filter.query.ddl = falsecanal.instance.filter.table.error = falsecanal.instance.filter.rows = falsecanal.instance.filter.transaction.entry = false# binlog format/image checkcanal.instance.binlog.format = ROW,STATEMENT,MIXED canal.instance.binlog.image = FULL,MINIMAL,NOBLOB# binlog ddl isolationcanal.instance.get.ddl.isolation = false# parallel parser configcanal.instance.parser.parallel = true## concurrent thread number, default 60% available processors, suggest not to exceed Runtime.getRuntime().availableProcessors()#canal.instance.parser.parallelThreadSize = 16## disruptor ringbuffer size, must be power of 2canal.instance.parser.parallelBufferSize = 256# table meta tsdb infocanal.instance.tsdb.enable = truecanal.instance.tsdb.dir = ${canal.file.data.dir:../conf}/${canal.instance.destination:}canal.instance.tsdb.url = jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL;canal.instance.tsdb.dbUsername = canalcanal.instance.tsdb.dbPassword = canal# dump snapshot interval, default 24 hourcanal.instance.tsdb.snapshot.interval = 24# purge snapshot expire , default 360 hour(15 days)canal.instance.tsdb.snapshot.expire = 360# aliyun ak/sk , support rds/mqcanal.aliyun.accessKey =canal.aliyun.secretKey =########################################################## destinations ##############################################################canal.destinations =# conf root dircanal.conf.dir = ../conf# auto scan instance dir add/remove and start/stop instancecanal.auto.scan = truecanal.auto.scan.interval = 5canal.instance.tsdb.spring.xml = classpath:spring/tsdb/h2-tsdb.xml#canal.instance.tsdb.spring.xml = classpath:spring/tsdb/mysql-tsdb.xmlcanal.instance.global.mode = managercanal.instance.global.lazy = falsecanal.instance.global.manager.address = ${canal.admin.manager}#canal.instance.global.spring.xml = classpath:spring/memory-instance.xmlcanal.instance.global.spring.xml = classpath:spring/file-instance.xml#canal.instance.global.spring.xml = classpath:spring/default-instance.xml########################################################### MQ ###############################################################canal.mq.servers = 127.0.0.1:6667canal.mq.retries = 0canal.mq.batchSize = 16384canal.mq.maxRequestSize = 1048576canal.mq.lingerMs = 100canal.mq.bufferMemory = 33554432canal.mq.canalBatchSize = 50canal.mq.canalGetTimeout = 100canal.mq.flatMessage = truecanal.mq.compressionType = nonecanal.mq.acks = all#canal.mq.properties. =canal.mq.producerGroup = test# Set this value to &quot;cloud&quot;, if you want open message trace feature in aliyun.canal.mq.accessChannel = local# aliyun mq namespace#canal.mq.namespace =########################################################### Kafka Kerberos Info ###############################################################canal.mq.kafka.kerberos.enable = falsecanal.mq.kafka.kerberos.krb5FilePath = &quot;../conf/kerberos/krb5.conf&quot;canal.mq.kafka.kerberos.jaasFilePath = &quot;../conf/kerberos/jaas.conf&quot;" }, { "title": "CentOS下的docker安装与配置", "url": "/posts/centos-docker-install&config/", "categories": "", "tags": "", "date": "2019-08-05 00:00:00 +0800", "snippet": "版本 CentOS 8.2docker-ce经测试yum里的docker包不能直接安装，直接安装的是podman-docker执行docker命令会抛出如下错误Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg./usr/bin/podman: symbol lookup error: /usr/bin/podman: undefined symbol: seccomp_notify_fd删除原有安装，重新安装阿里源的docker-cesudo yum remove docker# 配置阿里docker-ce源sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# 更新yum包sudo yum makecache fast# 安装docker-cesudo yum -y install docker-ce# 启动 docker 服务，否则docker相关命令无法执行sudo systemctl start docker## 设置 docker 服务自启动sudo systemctl enable docker此时运行docker命令会提示权限不够，使用sudo倒是可以了，但是可以通过简单的配置让普通账户也能执行docker命令，这里以admin账户为例，要求设置的账户有sudo权限# 查看docker用户sudo cat /etc/group | grep docker# 如果显示如下信息表示存在docker用户，一般正常安装docker-ce都会创建这个用户# docker:x:987ll /var/run/docker.sock# 显示如下# srw-rw----, 1 root docker 0 8月 5 20:31 /var/run/docker.sock# 将当前用户admin添加至docker组sudo gpasswd -a admin docker# 刷新组信息newgrp docker# 查看当前账户所属组信息id# uid=1000(admin) gid=987(docker) groups=987(docker),10(wheel),1000(admin) 再用该用户输入docker相关命令就不再有权限提示了" }, { "title": "数仓指标体系建设", "url": "/posts/data-index-system/", "categories": "Data Warehouse", "tags": "", "date": "2019-07-11 17:11:00 +0800", "snippet": "为什么要建立指标体系？数据指标体系肯定因为存在业务痛点而产生的需求，我们需要看一下痛点是否与指标体系能解决的属于同一问题域痛点分析从 业务、技术、产品 三个角度来看业务视角业务分析场景、指标、维度不明解数据需频繁变更，数据口径不统一、各业务系统数据参差不齐用户分析具体业务问题找数据，核对数据成本高技术视角指标定义和命名混乱，没有一个统一认知，指标不唯一，指标维护口径不一致，业务团队和数据团队有认识偏差指标生产，重复建设，数据汇算成本高指标消费、出口不统一，重复输出，输出口径不一致产品视角缺乏系统产品化支持从生产到消费数据流没有系统产品层面打通管理目标技术目标统一指标和维度管理、指标命令、指标口径、数据来源，维度定义规范、维度值一致业务目标统一数据出口、统一场景覆盖产品目标指标体系管理工具落地，指标体系内容产品化落地支持决策、分析、运营模型架构 划分业务板块 制定规范定义 数据域 修饰类型 - 修饰词 - 派生指标 业务过程 - 原子指标/度量 维度 - 维度属性 模型设计 汇总事实表 明细维度事实表 维度表 " }, { "title": "解决文件已被macOS使用无法打开", "url": "/posts/mac-openfile/", "categories": "Mac, tip", "tags": "", "date": "2018-10-11 13:40:00 +0800", "snippet": "在mac系统使用过程中有时会遇到想打开的文件无法打开的情况，这时候系统会提示 已被macOS使用,不能打开按如下方式操作即可解决 选中想要打开的文件，按一下copy快捷键 Command + c 打开应用程序终端 有些系统版本名称为 terminal 输入 xattr -d com.apple.FinderInfo ，注意最后有一个空格 保持输入焦点在终端窗口中，按下粘贴快捷键 Command + v，这里刚刚复制的文件路径会粘贴到窗口中 按下 Enter 执行输入的命令这样再去打开刚刚的文件，就能成功打开了" }, { "title": "Kafka 命令实战2", "url": "/posts/kafka-command/", "categories": "Big data, Kafka", "tags": "command", "date": "2018-06-22 01:29:00 +0800", "snippet": "Kafka 迭代很快，新版本又有不少变化，同事告诉我升级好了新集群，马上来练练测试环境zookeeper:服务器名称 mt-zookeeper-vip:218110.77.32.90 mt-zookeeper-410.77.32.91 mt-zookeeper-510.77.32.92 mt-zookeeper-610.77.32.2 mt-zookeeper-vipbroker-server10.7.31.15:2181,10.7.31.16:2181,10.7.31.17:218110.7.31.15:9092,10.7.31.16:9092,10.7.31.17:9092生产环境zookeeper服务器名称 mt-zookeeper-vip:2181broker-server10.33.36.101:9092,10.33.36.113:9092,10.33.40.117:9092PLAINTEXT://10.33.36.101:9092,PLAINTEXT://10.33.36.113:9092,PLAINTEXT://10.33.40.117:9092常用命令cd kafka/# 显示 topic 列表bin/kafka-topics.sh --list \\ --zookeeper mt-zookeeper-vip:2181# 创建一个 topicbin/kafka-topics.sh --create \\ --zookeeper mt-zookeeper-vip:2181 \\ --replication-factor 3 \\ --partitions 1 \\ --topic __connect-offsets# 删除一个 topicbin/kafka-topics.sh --delete \\ --zookeeper mt-zookeeper-vip:2181 \\ --topic __connect-offsets# 消费 kafka 消息，如果加上 --from-beginning 参数则从最早消息开始消费，# 否则由kafka记录上次位置后开始bin/kafka-console-consumer.sh \\ --zookeeper mt-zookeeper-vip:2181 \\ --topic bigDatamarket [--from-beginning]# 以交互方式发送一条 kafka 消息bin/kafka-console-producer.sh \\ --broker-list [broker server] \\ --topic [topic name]# 查看某 topic 当前 offset 值bin/kafka-run-class.sh kafka.tools.GetOffsetShell \\ --broker-list [broker server] \\ --topic lbs_test设置offset# ...zookeeper.connect=mt-zookeeper-vip:2181group.id=dmp_stream# ...命令行执行如下bin/kafka-run-class.sh \\ kafka.tools.UpdateOffsetsInZK \\ latest \\ consumer.properties \\ dmp_task_resultapache canal 推送的mysql记录更新消息结构{ &quot;dmlType&quot;:&quot;update&quot;, &quot;pkMap&quot;:{ &quot;partyManageId&quot;:&quot;20765&quot; }, &quot;normalMap&quot;:{ &quot;goodsSourceCount&quot;:&quot;1612&quot;, &quot;stampDate&quot;:&quot;2017-07-27 10:11:29&quot;, &quot;updateDate&quot;:&quot;2017-07-27 10:11:29&quot;, &quot;goldDriverStatus&quot;:&quot;已开通&quot; }}" }, { "title": "Apache DRUID 部署及使用", "url": "/posts/druid-started/", "categories": "Big data, Druid", "tags": "", "date": "2017-11-06 20:55:00 +0800", "snippet": "初始设置修改时区druid/conf/druid/broker/jvm.config 里的 UTC 修改为 Asia/ShanghaiHDFS数据加载支持修改 druid/conf/druid/_common/common.runtime.properties , 确保 druid.extensions.loadList 中有 druid-hdfs-storage，在 druid/conf/druid/_common/ 目录中添加以下hadoop配置文件 的软链接 core-site.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xml启动分布式服务测试环境示例DRUID 部署在同一服务器 jt-host-kvm-53# 启动 historicalnohup java `cat conf-quickstart/druid/historical/jvm.config | xargs` \\ -cp &quot;conf-quickstart/druid/_common:conf-quickstart/druid/historical:lib/*&quot; \\ io.druid.cli.Main server historical &amp;gt; log/historical.log 2&amp;gt;&amp;amp;1 &amp;amp;# 启动 brokernohup java `cat conf-quickstart/druid/broker/jvm.config | xargs` \\ -cp &quot;conf-quickstart/druid/_common:conf-quickstart/druid/broker:lib/*&quot; \\ io.druid.cli.Main server broker &amp;gt; log/broker.log 2&amp;gt;&amp;amp;1 &amp;amp;# 启动 coordinatornohup java `cat conf-quickstart/druid/coordinator/jvm.config | xargs` \\ -cp &quot;conf-quickstart/druid/_common:conf-quickstart/druid/coordinator:lib/*&quot; \\ io.druid.cli.Main server coordinator &amp;gt; log/coordinator.log 2&amp;gt;&amp;amp;1 &amp;amp;# 启动 overlordnohup java `cat conf-quickstart/druid/overlord/jvm.config | xargs` \\ -cp &quot;conf-quickstart/druid/_common:conf-quickstart/druid/overlord:lib/*&quot; \\ io.druid.cli.Main server overlord &amp;gt; log/overlord.log 2&amp;gt;&amp;amp;1 &amp;amp;# 启动 middleManagernohup java `cat conf-quickstart/druid/middleManager/jvm.config | xargs` \\ -cp &quot;conf-quickstart/druid/_common:conf-quickstart/druid/middleManager:lib/*&quot; \\ io.druid.cli.Main server middleManager &amp;gt; log/middleManager.log 2&amp;gt;&amp;amp;1 &amp;amp;集群部署示例参照测试环境命令，将进程部署到不同服务器，本例资源受限根据服务负载进行了部分隔离 hosts role port big-elephant-1 kafka   big-elephant-4 broker,overlord   big-elephant-5 historical,coordinator,middleManager   启动实时数据消费进程nohup bin/tranquility kafka -configFile conf/yugong-status.json &amp;amp;查看消费进程 Logtail -f http://big-elephant-8:8090/druid/indexer/v1/task/[taskid]/log扩展工具Druid Dumbohttps://github.com/liquidm/druid-dumbo 在HDFS中验证实时生成的原始数据。 重建与HDFS中原始数据不一致的细分。 将现有的段折叠成更高的粒度段。示例配置文件这个示例从kafka消息系统中消费消息，将消息中的值以分钟为时间窗口单位，metric_code字段为维度，metric_value为度量字段进行值统计，统计包括最大、最小、合计、总记录数，相当于增强版本 WordCount 程序。{ &quot;dataSources&quot;: [{ &quot;spec&quot;: { &quot;dataSchema&quot;: { &quot;dataSource&quot;: &quot;metric_append&quot;, &quot;parser&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;parseSpec&quot;: { &quot;timestampSpec&quot;: { &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;column&quot;: &quot;timestamp&quot; }, &quot;dimensionsSpec&quot;: { &quot;dimensions&quot;: [&quot;metric_code&quot;] }, &quot;columns&quot;: [&quot;metric_code&quot;, &quot;metric_value&quot;, &quot;timestamp&quot;] } }, &quot;granularitySpec&quot;: { &quot;type&quot;: &quot;uniform&quot;, &quot;queryGranularity&quot;: &quot;minute&quot; }, &quot;metricsSpec&quot;: [{ &quot;type&quot;: &quot;count&quot;, &quot;name&quot;: &quot;count&quot; }, { &quot;type&quot;: &quot;doubleSum&quot;, &quot;name&quot;: &quot;metric_sum&quot;, &quot;fieldName&quot;: &quot;metric_value&quot; }, { &quot;type&quot;: &quot;doubleMin&quot;, &quot;name&quot;: &quot;metric_min&quot;, &quot;fieldName&quot;: &quot;metric_value&quot; }, { &quot;type&quot;: &quot;doubleMax&quot;, &quot;name&quot;: &quot;metric_max&quot;, &quot;fieldName&quot;: &quot;metric_value&quot; }] }, &quot;tuningConfig&quot;: { &quot;maxRowsInMemory&quot;: &quot;10000&quot;, &quot;type&quot;: &quot;realtime&quot;, &quot;windowPeriod&quot;: &quot;PT30M&quot;, &quot;intermediatePersistPeriod&quot;: &quot;PT30M&quot; } }, &quot;properties&quot;: { &quot;topicPattern.priority&quot;: &quot;1&quot;, &quot;topicPattern&quot;: &quot;YugongMetricAppend&quot; } }, { &quot;spec&quot;: { &quot;dataSchema&quot;: { &quot;dataSource&quot;: &quot;metric_complete&quot;, &quot;parser&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;parseSpec&quot;: { &quot;timestampSpec&quot;: { &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;column&quot;: &quot;timestamp&quot; }, &quot;dimensionsSpec&quot;: { &quot;dimensions&quot;: [&quot;metric_code&quot;] }, &quot;columns&quot;: [&quot;metric_code&quot;, &quot;metric_value&quot;, &quot;timestamp&quot;] } }, &quot;granularitySpec&quot;: { &quot;type&quot;: &quot;uniform&quot;, &quot;queryGranularity&quot;: &quot;minute&quot; }, &quot;metricsSpec&quot;: [{ &quot;type&quot;: &quot;count&quot;, &quot;name&quot;: &quot;count&quot; }, { &quot;type&quot;: &quot;doubleSum&quot;, &quot;name&quot;: &quot;metric_sum&quot;, &quot;fieldName&quot;: &quot;metric_value&quot; }, { &quot;type&quot;: &quot;doubleMin&quot;, &quot;name&quot;: &quot;metric_min&quot;, &quot;fieldName&quot;: &quot;metric_value&quot; }, { &quot;type&quot;: &quot;doubleMax&quot;, &quot;name&quot;: &quot;metric_max&quot;, &quot;fieldName&quot;: &quot;metric_value&quot; }] }, &quot;tuningConfig&quot;: { &quot;maxRowsInMemory&quot;: &quot;10000&quot;, &quot;type&quot;: &quot;realtime&quot;, &quot;windowPeriod&quot;: &quot;PT30M&quot;, &quot;intermediatePersistPeriod&quot;: &quot;PT30M&quot; } }, &quot;properties&quot;: { &quot;topicPattern.priority&quot;: &quot;1&quot;, &quot;topicPattern&quot;: &quot;YugongMetricComplete&quot; } }, { &quot;spec&quot;: { &quot;dataSchema&quot;: { &quot;dataSource&quot;: &quot;yugong_writers&quot;, &quot;parser&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;parseSpec&quot;: { &quot;format&quot;: &quot;json&quot;, &quot;timestampSpec&quot;: { &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;, &quot;column&quot;: &quot;createtime&quot; }, &quot;dimensionsSpec&quot;: { &quot;dimensions&quot;: [&quot;metricCode&quot;, &quot;metricDate&quot;] } } }, &quot;granularitySpec&quot;: { &quot;type&quot;: &quot;uniform&quot;, &quot;queryGranularity&quot;: &quot;minute&quot; }, &quot;metricsSpec&quot;: [{ &quot;type&quot;: &quot;count&quot;, &quot;name&quot;: &quot;count&quot; }] }, &quot;tuningConfig&quot;: { &quot;maxRowsInMemory&quot;: &quot;10000&quot;, &quot;type&quot;: &quot;realtime&quot;, &quot;windowPeriod&quot;: &quot;PT30M&quot;, &quot;intermediatePersistPeriod&quot;: &quot;PT30M&quot; } }, &quot;properties&quot;: { &quot;topicPattern.priority&quot;: &quot;1&quot;, &quot;topicPattern&quot;: &quot;YugongWrite*&quot; } }], &quot;properties&quot;: { &quot;zookeeper.connect&quot;: &quot;10.33.35.192:2181,10.33.35.194:2181,10.33.35.196:2181&quot;, &quot;zookeeper.timeout&quot;: &quot;PT20S&quot;, &quot;druid.selectors.indexing.serviceName&quot;: &quot;druid/overlord&quot;, &quot;druid.discovery.curator.path&quot;: &quot;/druid/discovery&quot;, &quot;kafka.zookeeper.connect&quot;: &quot;10.33.35.192:2181,10.33.35.194:2181,10.33.35.196:2181&quot;, &quot;kafka.group.id&quot;: &quot;tranquility-k&quot;, &quot;consumer.numThreads&quot;: &quot;5&quot;, &quot;commit.periodMillis&quot;: &quot;15000&quot;, &quot;reportDropsAsExceptions&quot;: &quot;false&quot; }}以SQL方式访问DURID数据首先下载所需包进行验证测试java -cp lib/calcite-core-1.2.0-incubating.jar:lib/avatica-1.8.0.jar:lib/calcite-linq4j-1.2.0-incubating.jar:lib/mysql-connector-java-5.1.25.jar:lib/sqlline-1.4.0-SNAPSHOT-jar-with-dependencies.jar sqlline.SqlLine连接druidjava -cp lib/jackson-annotations-2.8.0.jar:lib/jackson-core-2.8.1.jar:lib/jackson-databind-2.8.1.jar:lib/guava-11.0.2.jar:lib/calcite-core-1.2.0-incubating.jar:lib/calcite-linq4j-1.2.0-incubating.jar:lib/calcite-avatica-1.2.0-incubating.jar:lib/calcite-druid-1.13.0.jar:lib/sqlline-1.4.0-SNAPSHOT-jar-with-dependencies.jar sqlline.SqlLine或者java -cp lib/avatica-1.10.0.jar:lib/calcite-core-1.13.0.jar:lib/calcite-druid-1.13.0.jar:lib/calcite-linq4j-1.13.0.jar:lib/commons-compiler-2.7.6.jar:lib/commons-lang3-3.2.jar:lib/guava-18.0.jar:lib/jackson-annotations-2.8.0.jar:lib/jackson-core-2.8.1.jar:lib/jackson-databind-2.8.1.jar:lib/janino-2.7.6.jar:lib/joda-time-2.8.1.jar:lib/sqlline-1.4.0-SNAPSHOT-jar-with-dependencies.jar sqlline.SqlLine然后!connect jdbc:calcite:schemaFactory=org.apache.calcite.adapter.druid.DruidSchemaFactory;schema.url=http://big-elephant-4:8082;schema.coordinatorUrl=http://big-elephant-5:8081;caseSensitive=mysql admin admin执行SQL查询select sum(count) as v from metric_append where __time &amp;gt; TIMESTAMP &#39;2017-08-10 00:00:00&#39; and metric_code=&#39;lj.publishcargo.any.amount&#39; group by metric_codeselect metric_sum from metric_complete where __time &amp;gt; TIMESTAMP &#39;2017-08-10 00:00:00&#39; and metric_code=&#39;lj.publishcargo.any.amount&#39; order by __time desc limit 1" }, { "title": "YARN 生产集群办公环境无法访问", "url": "/posts/yarn-rest-api/", "categories": "Big data, Yarn", "tags": "security", "date": "2017-08-30 19:40:00 +0800", "snippet": "公司出于安全考虑，陆续在办公网络禁用对生产系统的直接访问，对生产系统所有操作必须先登录跳板机，再通过跳板机对生产系统访问。原来直接访问Yarn Web UI来跟踪任务执行情况的方式已经无用了。幸好Yarn提供了基于HTTP Api的方式访问，下面开始搞起… 根据官网文档要求编写json格式内容存储到json文件，这里名为 yarn-rest-request.json 将该文件保存到跳板机 scp yarn-rest-request.json admin@[server_host]:~/ 在跳板机上执行如下命令，这里的域名及端口修改为实际域名和端口 curl -L -H&#39;Content-Type: application/json&#39; \\ -XPOST \\ --data-binary @yarn-rest-request.json \\ http://big-elephant-3:8088/ws/v1/cluster/apps 然后就可以看到服务器返回的信息了，如有必要可以将返回结果保存到文件便于后续使用。 如果跳板机无法保存文件执行命令等操作，使用iTerm2软件配置好隧道穿透即可上传yarn-rest-request.json文件到生产服务器，后面按照第3步开始执行即可" }, { "title": "YARN Web UI 不能查看", "url": "/posts/yarn-unauthorized/", "categories": "Big data, Yarn", "tags": "", "date": "2017-08-30 19:40:00 +0800", "snippet": "访问YARN Web UI地址时，页面出现如下提示：You (User dr.who) are not authorized to view application application_xxxx经多次翻日志，翻文档后，得知需要关注如下配置项，与实际环境是否相符。Hadoop运维过程中不经意间的配置修改就可能引发种种问题。&amp;lt;property&amp;gt;   &amp;lt;name&amp;gt;hadoop.http.staticuser.user&amp;lt;/name&amp;gt;   &amp;lt;value&amp;gt;hadoop&amp;lt;/value&amp;gt;  &amp;lt;/property&amp;gt; " }, { "title": "HBase 命令实战", "url": "/posts/hbase-command/", "categories": "Big data, HBase", "tags": "command", "date": "2017-07-10 09:40:00 +0800", "snippet": "运维告诉我，HBase集群已经搭建好了，现在来实战一把namespace 名为 hbase 的 namespace：系统内建表，包括 namespace 和 meta 表 名为 default 的 namespace：用户建表时未指定 namespace 的表都创建在此-- 创建namespacecreate_namespace &#39;ai_ns&#39;-- 删除namespacedrop_namespace &#39;ai_ns&#39;-- 查看namespacedescribe_namespace &#39;ai_ns&#39;-- 列出所有namespacelist_namespace-- 在namespace下创建表create &#39;ai_ns:testtable&#39;, &#39;fm1&#39;-- 查看namespace下的表list_namespace_tables &#39;ai_ns&#39;DDL-- 查看有哪些表list-- 创建表create &#39;YG_ODS:PARTY&#39;,{NAME =&amp;gt; &#39;0&#39;, VERSIONS =&amp;gt; 1},{NAME =&amp;gt; &#39;1&#39;, VERSIONS =&amp;gt; 2}-- 删除表disable &#39;t1&#39;drop &#39;t1&#39;-- 查看表结构describe &#39;t1&#39;-- 修改表结构（修改表结构必须先disable）disable &#39;test1&#39;alter &#39;test1&#39;,{NAME=&amp;gt;&#39;body&#39;,TTL=&amp;gt;&#39;15552000&#39;},{NAME=&amp;gt;&#39;meta&#39;, TTL=&amp;gt;&#39;15552000&#39;}enable &#39;test1&#39;privileges-- 语法 : grant &amp;lt;user&amp;gt; &amp;lt;permissions&amp;gt; &amp;lt;table&amp;gt; &amp;lt;column family&amp;gt; &amp;lt;column qualifier&amp;gt; 参数后面用逗号分隔-- 权限用五个字母表示： &quot;RWXCA&quot;.-- READ(&#39;R&#39;), WRITE(&#39;W&#39;), EXEC(&#39;X&#39;), CREATE(&#39;C&#39;), ADMIN(&#39;A&#39;)-- 例如，给用户‘test&#39;分配对表t1有读写的权限，grant &#39;test&#39;,&#39;RW&#39;,&#39;t1&#39;-- 查看权限-- 语法：user_permission &amp;lt;table&amp;gt;-- 例如，查看表t1的权限列表user_permission &#39;t1&#39;-- 收回权限-- 与分配权限类似，语法：revoke &amp;lt;user&amp;gt; &amp;lt;table&amp;gt; &amp;lt;column family&amp;gt; &amp;lt;column qualifier&amp;gt;-- 例如，收回test用户在表t1上的权限revoke &#39;test&#39;,&#39;t1&#39;DML-- 添加数据-- 语法：put &amp;lt;table&amp;gt;,&amp;lt;rowkey&amp;gt;,&amp;lt;family:column&amp;gt;,&amp;lt;value&amp;gt;,&amp;lt;timestamp&amp;gt;-- 例如：给表t1的添加一行记录：rowkey是rowkey001，family name：f1，column name：col1，value：value01，timestamp：系统默认put &#39;t1&#39;,&#39;rowkey001&#39;,&#39;f1:col1&#39;,&#39;value01&#39;-- 查询某行记录-- 语法：get &amp;lt;table&amp;gt;,&amp;lt;rowkey&amp;gt;,[&amp;lt;family:column&amp;gt;,....]-- 例如：查询表t1，rowkey001中的f1下的col1的值get &#39;t1&#39;,&#39;rowkey001&#39;, &#39;f1:col1&#39;-- 或者：get &#39;t1&#39;,&#39;rowkey001&#39;, {COLUMN=&amp;gt;&#39;f1:col1&#39;}-- 查询表t1，rowke002中的f1下的所有列值get &#39;t1&#39;,&#39;rowkey001&#39;扫描表-- 语法：scan &amp;lt;table&amp;gt;, {COLUMNS =&amp;gt; [ &amp;lt;family:column&amp;gt;,.... ], LIMIT =&amp;gt; num}-- 另外，还可以添加STARTROW、TIMERANGE和FITLER等高级功能-- 例如：扫描表t1的前5条数据scan &#39;t1&#39;,{LIMIT=&amp;gt;5}-- 查询表中的数据行数-- 语法：count &amp;lt;table&amp;gt;, {INTERVAL =&amp;gt; intervalNum, CACHE =&amp;gt; cacheNum}-- INTERVAL设置多少行显示一次及对应的rowkey，默认1000；CACHE每次去取的缓存区大小，默认是10，调整该参数可提高查询速度-- 例如，查询表t1中的行数，每100条显示一次，缓存区为500count &#39;t1&#39;, {INTERVAL =&amp;gt; 10000, CACHE =&amp;gt; 50000}-- 删除行中的某个列值-- 语法：delete &amp;lt;table&amp;gt;, &amp;lt;rowkey&amp;gt;,  &amp;lt;family:column&amp;gt; , &amp;lt;timestamp&amp;gt;,必须指定列名-- 例如：删除表t1，rowkey001中的f1:col1的数据delete &#39;t1&#39;,&#39;rowkey001&#39;,&#39;f1:col1&#39;-- 删除行-- 语法：deleteall &amp;lt;table&amp;gt;, &amp;lt;rowkey&amp;gt;,  &amp;lt;family:column&amp;gt; , &amp;lt;timestamp&amp;gt;，可以不指定列名，删除整行数据-- 例如：删除表t1，rowk001的数据deleteall &#39;t1&#39;,&#39;rowkey001&#39;-- 删除表中的所有数据-- 语法： truncate &amp;lt;table&amp;gt;-- 其具体过程是：disable table -&amp;gt; drop table -&amp;gt; create table-- 例如：删除表t1的所有数据truncate &#39;t1&#39;-- 查询指定rowKey和指定列数据get &#39;mofang:metric_today&#39;, &quot;\\x03\\xBC\\xD5\\x9E03dd85fb223cb2cbc203e518059aa558&quot;, { COLUMNS =&amp;gt; [ &#39;base&#39;, &#39;minutes&#39; ], FILTER =&amp;gt; &quot;ColumnRangeFilter(&#39;1111&#39;,true,&#39;1115&#39;,true)&quot;, LIMIT=&amp;gt;20 }-- 过滤器使用scan &#39;mofang:metric_today&#39;, { COLUMNS =&amp;gt; [ &#39;base&#39;, &#39;minutes&#39; ], FILTER =&amp;gt; &quot;ColumnRangeFilter(&#39;1111&#39;,true,&#39;1115&#39;,true)&quot;, LIMIT=&amp;gt;20 }Sqoop导入数据到HBasesqoop import \\ --connect &#39;jdbc:mysql://10.7.13.48:8066/trade?tinyInt1isBit=false&amp;amp;useCompression=true&amp;amp;tcpRcvBuf=1024000000&amp;amp;useCursorFetch=true&amp;amp;defaultFetchSize=1000&#39; \\ --table Party \\ --hbase-create-table \\ --hbase-table ODS_PARTY:PARTY \\ --column-family 1 \\ --hbase-row-key partyId \\ --split-by stampDate \\ --username &#39;admin&#39; \\ --password ****** \\ -m 5sqoop import \\ --connect &#39;jdbc:mysql://10.33.60.38:3306/dataMarket?tinyInt1isBit=false&amp;amp;useCompression=true&amp;amp;tcpRcvBuf=1024000000&amp;amp;useCursorFetch=true&amp;amp;defaultFetchSize=1000&#39; \\ --table Person \\ --hbase-create-table \\ --hbase-table ODS_PARTY:PERSON \\ --column-family 1 \\ --hbase-row-key partyId \\ --split-by stampDate \\ --username &#39;dataMarket&#39; \\ --password ****** \\ -m 5执行完成后查看表结构root|-- partyId: integer (nullable = false)|-- partyType: string (nullable = true)|-- partyName: string (nullable = true)|-- mobileNumber: string (nullable = true)|-- mobileNumberIsActive: string (nullable = true)|-- email: string (nullable = true)|-- emailIsActive: string (nullable = true)|-- tradeType: string (nullable = true)|-- oldPartyId: integer (nullable = true)|-- starLevel: string (nullable = true)|-- inputDate: timestamp (nullable = true)|-- createDate: timestamp (nullable = true)|-- status: string (nullable = true)|-- stampDate: timestamp (nullable = false)" }, { "title": "Spark 程序优化一例", "url": "/posts/spark-sql-performance/", "categories": "Big data, Spark", "tags": "performance", "date": "2017-07-10 09:40:00 +0800", "snippet": "Spark SQL 很🔥，官方宣称开发效率大幅提高，并且程序逻辑更容易理解，然而实际使用起来却也有运行效率低下，优化手段匮乏等缺点。传统数据库采用SQL，提高查询速度很大程度上依赖于索引手段进行SQL优化，而Spark SQL中的表无法使用索引技术。Spark 应用基于内存计算，对于聚合类型计算 shuffle 过程难以避免，此时数据传输速度下降几个数量级。应着重减少 shuffle 次数以及 shuffle 数据量。支撑公司当前业务的 Spark 程序计算速度不达预期，因此着手进行优化。优化目标稳定性，能力，速度，资源占用优化结果每5分钟时间窗口数据计算时间由 &amp;gt;90s 下降至 &amp;lt;30s实际优化策略 删除数据类，减少 shuffle 过程中序列化反序列化耗时，同时减少内存占用，删除BigScreenIndex case class transformation 阶段即过滤掉不参与计算的数据，即优先过滤数据。例：过滤 dmlType = delete 或 dmlType = remove 的数据 减少计算次数，重用计算中间结果。例：业务sql 与 topic对应关系只需要计算一次 尽量减少spark sql的使用，因为 spark sql 为动态编译加载技术，运行上需要走分析器、优化器、转换为原生API那套逻辑，最后实现还是基于RDD数据结构，做为7x24小时运行的应用来说，节省的那点开发时间微不足道，示例：由于SQL语法限制，原有程序逻辑使用了5条SQL才实现业务需求，改为 streaming 处理代码后，仅1次数据转换就得到所属数据结构详细业务逻辑及优化过程 业务数据计算 增量数据 topic list: select distinct topic from event_table 增量数据合并 SELECT title, type, smalltype, sum(valuelong) as valuelong FROM ( SELECT title, type, smalltype, valuelong FROM old_table UNION ALL SELECT title, type, smalltype, valuelong from increment_table ) t GROUP BY title, type, smalltype 指标比率计算 SELECT t1.title, t1.type, t1.smalltype, t1.valuelong, round((t1.valuelong / t2.valuelong) * 100) as valuedouble from merged_table t1 LEFT JOIN ( select t.type, nvl(t.smalltype,&#39;&#39;) as smalltype, sum(valuelong) as valuelong from merged_table t where t.type is not null group by t.type, t.smalltype ) t2 on t1.type = t2.type and nvl(t1.smalltype,&#39;&#39;) = t2.smalltype 新增及变更记录区分 select o.bigscreendataid, n.title, n.type, n.smalltype, n.valuelong, n.valuedouble, if(o.bigscreendataid is null, &#39;append&#39;, if(n.valuelong!=o.valuelong, &#39;changed&#39;, if(nvl(n.valuedouble, 0) != nvl(o.valuedouble,0), &#39;changed&#39;, &#39;none&#39;) ) ) as status from new_table n left join jdbc_table o on nvl(n.title,&#39;&#39;) = nvl(o.title,&#39;&#39;) and nvl(n.type, &#39;&#39;) = nvl(o.type, &#39;&#39;) and nvl(n.smalltype, &#39;&#39;) = nvl(o.smalltype, &#39;&#39;) 优化后的代码 public class BigScreenData() { public BigScreenData() {}} 心得 在一次map阶段即转换好所需要的字段数据，以便重用，而不是在用到的时候再计算，造成多次计算, 例如id, timestamp, 在转换出new RDD[Row]时就计算好了，在loadHDFS, 和loadDB中直接使用，而非在loadDB中再计算一次 实现效果相同前提下，选择更优算法 关注spark ui, 对运行参数调整, driver, executor, memory, cores, receiver.maxRate" }, { "title": "Apache Sqoop 命令实战", "url": "/posts/sqoop-command/", "categories": "Big data, Sqoop", "tags": "command", "date": "2016-08-13 11:29:00 +0800", "snippet": "最近 sqoop 用得比较多，在此把用到的命令记录一下官方文档地址 1.4.6：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html创建 Sqoop 任务sqoop job --create append-import-goods_source_search -- \\ --options-file sqoop-conf/import-hdfs.conf \\ --append \\ --table GoodsSourceSearch \\ --target-dir /user/hive/warehouse/mirror.db/goods_source_search/ \\ --check-column inputDate \\ --incremental append \\ --password ******创建 Sqoop 全量任务，执行时覆盖已有数据sqoop job --create import-auth_user -- \\ --options-file sqoop-conf/import-hive.conf \\ --table AuthUser \\ --hive-table auth_user \\ --password ******Sqoop 增量导入sqoop job --create append-import-call_record -- \\ --options-file sqoop-conf/import-hdfs.conf \\ --append \\ --table CallRecord \\ --target-dir /user/hive/warehouse/mirror.db/call_record/ \\ --check-column inputDate \\ --incremental append \\ --password ******导入文件格式sqoop的hive导入低层文件类型只支持textfile格式，hdfs导入则支持textfile、sequencefile、avrofile三种格式，因此如果希望导入数据到hive表最简单的是选择textfile格式，如果对hive表数据处理性能有要求需要至少两步操作，如果hive表选择sequencefile或avrofile格式，可以参数指定对应类型后使用hdfs导入，然后hive表刷新元数据使其加载新导入文件，如果使用rcfile/orcfile格式，可textfile格式导入表后用HIVE语句”insert into table testtable select * from textfile_table”导入 或者开发Sqoop扩展完成对格式支持 使用avrofile等格式时，其数据表结构信息是存储在该Hive元数据表字段中的，MySQL的varchar2类型字段有最大长度，当数据表的字段多了以后整个数据表结构描述信息的字符串长度将超出MySQL varchar2类型字段最大长度，而且由于MySQL中行记录数据长度也存在限制，实际数据表结构描述可用长度远远不到varchar2最大长度上限，字段多的数据表将在建表时因结构描述信息被截断无法正常解析，实践中此问题多发于Sqoop自动导入建表过程中。分析以上两种方案，其中sequencefile和avrofile格式io等资源消耗较少，只需要写一次hdfs然后加载数据即可，rcfile,orcfile相比较而言多一次格式转换和写文件操作，如果表需要多次查询使用则牺牲导入时间换取后续表数据使用时的高效 同数据量文件大小排序：sequencefile &amp;gt; textfile &amp;gt; avrofile &amp;gt; rcfile 同数据量查询负载排序：textfile &amp;gt; sequencefile &amp;gt; avrofile &amp;gt; rcfile创建sqoopJob追加数据到HDFS,同时对已有数据做更新:sqoop job --create increment-import-repport-goods_source --meta-connect jdbc:hsqldb:hsql://big-bigworker-6.100.idc.tf56:16000/sqoop -- --options-file sqoop-conf/import-hdfs.conf --table GoodsSource --target-dir /user/hive/warehouse/increment.db/report_goods_source/ --merge-key goodsSourceId --check-column updateDate --incremental lastmodified --password ******创建sqoopJob只是追加数据到HDFS,不对会已有数据做更新sqoop job --create append-import-party_log -- --options-file sqoop-conf/import-hdfs.conf --append --table PartyLog --target-dir /user/hive/warehouse/mirror.db/party_log/ --check-column inputDate --incremental append --password ******sqoop job --list --meta-connect jdbc:hsqldb:hsql://big-bigworker-6.100.idc.tf56:16000/sqoop sqoop job --create increment-export-report-log_yunbao_lujing_app_dau -- --options-file sqoop-conf/export.conf --table QLujingAppDriverAndShipperDau --update-key inputDate --update-mode allowinsert --export-dir /hive/warehouse/increment.db/report_lujing_app_dau sqoop job --create merge-import-repport-red_packet_detail --meta-connect jdbc:hsqldb:hsql://big-bigworker-6.100.idc.tf56:16000/sqoop -- --options-file sqoop-conf/import-hdfs.conf --table DimParty --target-dir /user/hive/warehouse/report.db/red_packet_detail/ --merge-key redPacketDetailId --check-column updateDate --incremental lastmodified --password ******" }, { "title": "Apache Flume Configuration", "url": "/posts/flume-configuration/", "categories": "Big data, Flume", "tags": "", "date": "2016-08-13 01:29:00 +0800", "snippet": "Flume 是大数据 Hadoop 生态的日志集成组件，通过它可以将其它服务器上的日志文件集成到 Hadoop HDFS、HIVE、ES等，Flume包含三大组件，source、 channel、sinksourcessources 负责数据的采集部分，在配置文件中以 “source.” 开头的即是相关配置，通常可以配置它的类型，采集后送往哪些 channels ，高阶点的可以配置 n 个拦截器，对采集到的数据进行过滤转换等，官方提供了常见的过滤器实现，直接配置相关参数即可，如无法满足需求可以自己编写过滤器实现更灵活功能。channelschannels 顾名思义是数据通道，采集到的数据由此发送到不同消费端，建议采用 memory 类型，其实类型均会不同程度降低吞量。sinkssinks 就是消费端，消费采集到的数据，通常情况存到 HDFS，通过对目录及文件地址的设计可以直接存为HIVE目录下的HDFS文件，比起 type=hive 的 sink 更快，但要注意使用前对HIVE元数据表进行刷新加载这些文件数据。featuresFlume集群收集的日志发送到hdfs上建立文件夹的时间依据是根据 event 时间，既日志传入Flume服务器的时间，源代码实现上是 Clock.unixTime()，所以如果想要根据日志自身生成的时间来建立文件夹的话，需要对 com.cloudera.flume.core.EventImpl 类的构造函数public EventImpl(byte[] s, long timestamp, Priority pri, long nanoTime, String host, Map&amp;lt;String, byte[]&amp;gt; fields)重写，解析方法参数 byte[] s 的内容取出时间，赋值给 timestamp，这样在 Flume sink 配置文件中 timestamp 才可以作为一个字段被识别和处理，由此解决根据日志生成时间存储等相关问题。 flume的框架会构造 byte[] s 长度为 0 的数组，用来发送类似简单验证的 event，所以在抽取日志生成时间时需要注意 s 长度为 0 的问题。HDFS Sink 配置实例############################################# producer config#############################################agent sectionproducer.sources = sproducer.channels = c c1 c2producer.sinks = r h es#source sectionproducer.sources.s.type =execproducer.sources.s.command = tail -f /usr/local/nginx/logs/test1.log#producer.sources.s.type = spooldir#producer.sources.s.spoolDir = /usr/local/nginx/logs/#producer.sources.s.fileHeader = trueproducer.sources.s.channels = c c1 c2producer.sources.s.interceptors = i#不支持忽略大小写producer.sources.s.interceptors.i.regex = .*\\.(css|js|jpg|jpeg|png|gif|ico).*producer.sources.s.interceptors.i.type = org.apache.flume.interceptor.RegexFilteringInterceptor$Builder#不包含producer.sources.s.interceptors.i.excludeEvents = true############################################# hdfs config############################################producer.channels.c.type = memory#Timeout in seconds for adding or removing an eventproducer.channels.c.keep-alive= 30producer.channels.c.capacity = 10000producer.channels.c.transactionCapacity = 10000producer.channels.c.byteCapacityBufferPercentage = 20producer.channels.c.byteCapacity = 800000producer.sinks.r.channel = cproducer.sinks.r.type = avroproducer.sinks.r.hostname = 127.0.0.1producer.sinks.r.port = 10101############################################# hdfs config############################################producer.channels.c1.type = memory#Timeout in seconds for adding or removing an eventproducer.channels.c1.keep-alive= 30producer.channels.c1.capacity = 10000producer.channels.c1.transactionCapacity = 10000producer.channels.c1.byteCapacityBufferPercentage = 20producer.channels.c1.byteCapacity = 800000producer.sinks.h.channel = c1producer.sinks.h.type = hdfs#目录位置producer.sinks.h.hdfs.path = hdfs://127.0.0.1/tmp/flume/%Y/%m/%d#文件前缀producer.sinks.h.hdfs.filePrefix=nginx-%Y-%m-%d-%Hproducer.sinks.h.hdfs.fileType = DataStream#时间类型必加，不然会报错producer.sinks.h.hdfs.useLocalTimeStamp = trueproducer.sinks.h.hdfs.writeFormat = Text#hdfs创建多长时间新建文件，0不基于时间#Number of seconds to wait before rolling current file (0 = never roll based on time interval)producer.sinks.h.hdfs.rollInterval=0#hdfs多大时新建文件，0不基于文件大小#File size to trigger roll, in bytes (0: never roll based on file size)producer.sinks.h.hdfs.rollSize = 0#hdfs有多少条消息时新建文件，0不基于消息个数#Number of events written to file before it rolled (0 = never roll based on number of events)producer.sinks.h.hdfs.rollCount = 0#批量写入hdfs的个数#number of events written to file before it is flushed to HDFSproducer.sinks.h.hdfs.batchSize=1000#flume操作hdfs的线程数（包括新建，写入等）#Number of threads per HDFS sink for HDFS IO ops (open, write, etc.)producer.sinks.h.hdfs.threadsPoolSize=15#操作hdfs超时时间#Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring.producer.sinks.h.hdfs.callTimeout=30000乱入几个 Hadoop HDFS 相关配置在 Flume 向 HDFS 系统持续写入数据时，HDFS的一些配置会影响实际的目录及文件生成，需要加入注意hdfs.round=false#Should the timestamp be rounded down (if true, affects all time based escape sequences except %t)hdfs.roundValue=#Rounded down to the highest multiple of this (in the unit configured using hdfs.roundUnit), less than current time.hdfs.roundUnit=&#39;second&#39;#The unit of the round down value - second, minute or hour.ES Sink 配置实例############################################# elasticsearch config############################################producer.channels.c2.type = memory#Timeout in seconds for adding or removing an eventproducer.channels.c2.keep-alive= 30producer.channels.c2.capacity = 10000producer.channels.c2.transactionCapacity = 10000producer.channels.c2.byteCapacityBufferPercentage = 20producer.channels.c2.byteCapacity = 800000producer.sinks.es.channel = c2producer.sinks.es.type = org.apache.flume.sink.elasticsearch.ElasticSearchSinkproducer.sinks.es.hostNames = 127.0.0.1:9300#Name of the ElasticSearch cluster to connect toproducer.sinks.es.clusterName = sunxucool#Number of events to be written per txn.producer.sinks.es.batchSize = 1000#The name of the index which the date will be appended to. Example ‘flume’ -&amp;gt; ‘flume-yyyy-MM-dd’producer.sinks.es.indexName = flume_es#The type to index the document to, defaults to ‘log’producer.sinks.es.indexType = testproducer.sinks.es.serializer = org.apache.flume.sink.elasticsearch.ElasticSearchLogStashEventSerializer" }, { "title": "ElasticSearch 命令实战", "url": "/posts/es-command/", "categories": "Big data, ElasticSearch", "tags": "command", "date": "2016-08-13 01:29:00 +0800", "snippet": "记录一下 elasticsearch 高频命令添加删除模版curl -XDELETE &#39;http://[server_host]:9200/goods-source-log&#39;curl -XPUT &#39;http://[server_host]:9200/_template/goods-source-log&#39; -d &#39;@goods-source-log-v1.json&#39;删除索引会将整个索引包括数据全部删除curl -XDELETE &#39;http://[server_host]:9200/goods-source-log&#39;查找数据curl -XPOST &#39;http://[server_host]:9200/goods-source-log/_search?q=*&amp;amp;pretty&#39;" }, { "title": "Hive 命令实战", "url": "/posts/hive-command/", "categories": "Big data, Hive", "tags": "command", "date": "2016-07-28 11:29:00 +0800", "snippet": "建表语句-- 创建表create table test(id int, name string);-- 创建外部表create external table test(id int, name string) location `/user/yundao/test_table`-- 指定行分隔 row format delimited-- 指定列分隔 fields terminated by &#39;\\t&#39;-- 指定分区 partitioned by (pt string)-- 指定存储格式，可选格式有-- textfile 文本格式, 默认值-- sequencefile Hadoop API 提供的一种二进制文件，它将数据以&amp;lt;key,value&amp;gt;的形式序列化到文件中，相比textfile空间占用小-- rcfile 一种列存的数据格式，在汇总计算时有性能加成，相比textfile空间占用小-- orcfile rcfile的升级版本 stored as textfile;-- 指定内容格式ROW FORMAT SERDE &#39;org.openx.data.jsonserde.JsonSerDe&#39;完整版例子create external table logs_user_event( `type` int, `time` timestamp, code int, address string, phonemode string, os string, serialnumber string, uuid string, mac string, version string, sourcecode string, ip string, partyid bigint) row format serde &quot;org.openx.data.jsonserde.JsonSerDe&quot; with serdeproperties ( &quot;ignore.malformed.json&quot; = &quot;true&quot;, &quot;dots.in.keys&quot; = &quot;true&quot;)location &quot;/logs/user_event&quot;修改表的SerDe属性ALTER TABLE lujing_adapp_burry_point_delay SET SERDEPROPERTIES ( &quot;ignore.malformed.json&quot; = &quot;true&quot;);显示表结构-- 显示数据库已有表show share.tables; -- 显示表描述desc tb_test;desc extended tb_test;desc formatted tb_test; -- 修改字段alter table tb_test change id id2 int comment &quot;注释&quot;;-- 修改表注释alter table tb_test set TBLPROPERTIES(&#39;comment&#39;=&#39;每天各线路找货会员表&#39;);加载数据-- 加载本地文件数据, 覆盖模式，声明格式与实际格式不同的数据加载后无法正确读取，例如数据格式为text格式，-- 表格式为sequencefile,rcfile,avrofile等，目前sqoop导入到hive只支持textfile格式，-- 导入到hdfs支持textfile,sequencefile,avrofile三种格式*load data local inpath &#39;/home/yundao/test_data.txt&#39; overwrite into table test;-- 加载本地文件数据, 追加模式load data local inpath &#39;/home/yundao/test_data.txt&#39; into table test;-- 加载HDFS数据, 覆盖模式load data inpath &#39;/user/yundao/test_data&#39; overwrite into table test;-- 加载HDFS数据, 追加模式load data inpath &#39;/user/yundao/test_data&#39; into table test;-- 从查询插入数据, 覆盖模式insert overwrite table test_target select * from test_source;-- 从查询插入数据, 追加模式insert into table test_target select * from test_source;-- 加载数据到分区，期中test_source表必须有与test_target分区同名列insert into table test_target PARTITION (pt=&#39;[分区名]&#39;) select * from test_source;-- 当test_source表没有名为pt的列时insert into table test_target PARTITION (pt=&#39;[分区名]&#39;) select *, &#39;2016-07-28&#39; as pt from test_source;-- 将查询结果插入多个表或HDFS目录from test_insert1insert overwrite local directory &#39;/home/yundao/hive&#39; select * insert overwrite directory &#39;/user/yundao/export_test&#39; select value;导出数据-- 导出到本地目录insert overwrite local directory &#39;party_route&#39; row format delimited fields terminated by &#39;\\t&#39; select * from party_route order by to_party_id, take_month desc, total_value desc;导出到hdfs目录只需要去掉第一行中的’local’关键字表分区(PARTITION)-- 创建分区表create table test1(dummy int) partitioned by(pt string); -- 创建多分区表create table test1(dummy int) partitioned by(ptm string, ptd string); -- 创建静态分区alter table test1 add partition (pt=&quot;201507&quot;);alter table test1 add partition (ptm=&quot;201507&quot;, ptd=&quot;01&quot;); -- 创建动态分区-- 从已有的数据加载并自动创建分区set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict; insert overwrite table test1 partition(pt) select *,substr(create_date,1,6) as pt from source_table; -- 列出表分区show partitions test1; -- 删除表分区alter table test1 drop partition(pt=&quot;201507&quot;);-- 函数&amp;amp;UDF/UDAF/UDTF-- 临时函数add jar hive-udf.jar;create temporary function tf as &#39;com.tf56.yundao.udf.PartySpeedUDF&#39;;-- 删除临时函数drop temporary function tf;delete jar hive-udf.jar; -- 列出已添加的jarlist jar; -- 永久函数(适用数据库范围内使用)use share;create function share.row_num as &#39;com.tf56.yundao.udf.RowNumberUDF&#39; using jar &#39;hdfs://ns1/user/yundao/udf.jar&#39;; -- 显示函数列表show functions; -- 显示函数描述desc function to_date;desc function extended to_date;查询排序三个关键字说明如下 distribute by：控制着在map阶段如何分区，按照什么字段进行分区，重点考虑均衡负载避免倾斜 sort by：每个reduce按照sort by 字段进行排序，reduce的数量按照默认的数量来进行，当然可以指定。最终可以进行归并排序得出结果。适用于数据量比较大的排序场景。 order by：reduce只有一个，在一个reduce中完成排序，使用于数据量小的场景或者对最后结果排序。mapjoin应用场景：1.关联操作中有一张表非常小 2.不等值的链接操作select /*+ mapjoin(A)*/ f.a,f.b from A t join B f on ( f.a=t.a and f.ftime=20110802)特殊数据结构Array、Map、Struct复合数据结构的常用函数-- array_contains - 数组中是否包含指定值select * from rc_coupon where array_contains([1,2,3,4], type);-- explode - 展开数组为行select explode(split(&#39;hello world&#39;, &#39; &#39;)); -- posexplode - 展开数组为行, 并添加序号列select posexplode(split(&#39;hello world&#39;, &#39; &#39;));view…常用内置函数字符串操作-- lpad - 左填充补齐长度select lpad(&#39;hi&#39;, 3, &#39;?&#39;) from dual; -- rpad - 右填充补齐长度select rpad(&#39;hi&#39;, 3 , &#39;.&#39;) from dual; -- sentences - 分词,将字符串分词为数组列表select sentences(&quot;hello world&quot;) from dual;select sentences(&quot;hello,world&quot;) from dual; -- str_to_map - 字符串转map，delimiter1 - map元素分隔符，delimiter2 - map键值分隔符str_to_map(text, delimiter1, delimiter2) - Creates a map by parsing textselect str_to_map(&#39;key1:val1,key2:val2&#39;, &#39;,&#39;, &#39;:&#39;) from dual; -- translate - 转换字符translate(&#39;abcdef&#39;, &#39;adc&#39;, &#39;19&#39;) returns &#39;1b9ef&#39; replacing &#39;a&#39; with &#39;1&#39;, &#39;d&#39; with &#39;9&#39; and removing &#39;c&#39; from the input stringselect translate(&#39;abcdef&#39;, &#39;adc&#39;, &#39;19&#39;) from dual;ucase, upper - 小字转大写lcase - 大写转小写bin - 2进制表示hex - 16进制表示xpath, xpath_boolean, xpath_double, xpath_float, xpath_int - 用于xml, html搜索计算的利器统计函数-- percentile - 百分值计算select percentile(price, array(0.3,0.5,0.7)) from trade;-- percentile_approx - 百分值计算(近似)select percentile_approx(price, array(0.3,0.5,0.7), 100000000) from trade;-- histogram_numeric - 直方图计算select histogram_numeric(price, 10) from trade; -- lag - 后一条记录值select p1.p_mfgr, p1.p_name, p1.p_size, p1.p_size - lag(p1.p_size,1,p1.p_size) over( distribute by p1.p_mfgr sort by p1.p_name) as delt from part p1 join part p2 on p1.p_partkey = p2.p_partkey -- lead - 前一条记录值select lead(id), id from dual; -- ngrams - 参见n-gram算法ngrams(array&amp;lt;&amp;gt;, int N, int K, int pf)-- 计算twitter中某两个词同时出现的频率的 top 100 --SELECT ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter; -- stack* - ??select stack(1, id) from dual; -- std - 计算标准差select std(column1) from dual;select stddev(column1) from dual;select stddev_pop(column1) from dual; -- var_pop, variance - 计算方差用于JSON的函数-- get_json_object - 字符串转json对象select get_json_object(line, &#39;$.type&#39;) from ext_passport limit 10; -- json_tuple - json字符串转tupleselect min(t.date), max(t.date) from ext_passport lateral view json_tuple(line, &#39;type&#39;, &#39;date&#39;) t as type, date;窗口函数-- sum, avg, min, max，关键是理解ROWS BETWEEN含义,也叫做WINDOW子句-- sum - 合计SELECT cookieid, createtime, pv, SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv1, -- 默认为从起点到当前行 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS pv2, --从起点到当前行，结果同pv1 SUM(pv) OVER(PARTITION BY cookieid) AS pv3, --分组内所有行 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv4, --当前行+往前3行 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) AS pv5, --当前行+往前3行+往后1行 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS pv6 ---当前行+往后所有行 FROM lxw1234;cookieid createtime pv pv1 pv2 pv3 pv4 pv5 pv6-----------------------------------------------------------------------------cookie1 2015-04-10 1 1 1 26 1 6 26cookie1 2015-04-11 5 6 6 26 6 13 25cookie1 2015-04-12 7 13 13 26 13 16 20cookie1 2015-04-13 3 16 16 26 16 18 13cookie1 2015-04-14 2 18 18 26 17 21 10cookie1 2015-04-15 4 22 22 26 16 20 8cookie1 2015-04-16 4 26 26 26 13 13 4-----------------------------------------------------------------------------各列计算逻辑说明： pv1: 分组内从起点到当前行的pv累积，如，11号的pv1=10号的pv+11号的pv, 12号=10号+11号+12号 pv2: 同pv1 pv3: 分组内(cookie1)所有的pv累加 pv4: 分组内当前行+往前3行，如，11号=10号+11号， 12号=10号+11号+12号， 13号=10号+11号+12号+13号， 14号=11号+12号+13号+14号 pv5: 分组内当前行+往前3行+往后1行，如，14号=11号+12号+13号+14号+15号=5+7+3+2+4=21 pv6: 分组内当前行+往后所有行，如，13号=13号+14号+15号+16号=3+2+4+4=13，14号=14号+15号+16号=2+4+4=10如果不指定ROWS BETWEEN,默认为从起点到当前行;如果不指定ORDER BY，则将分组内所有值累加;关键是理解ROWS BETWEEN含义,也叫做WINDOW子句：PRECEDING：往前FOLLOWING：往后CURRENT ROW：当前行UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING：表示到后面的终点其他AVG，MIN，MAX，和SUM用法一样。-- avg - 均值，商品日均价变化曲线select goods_id, avg(price) over(partition by goods_id,to_date(create_time) order by create_time) as price_avgfrom goods-- min - 最小值，商品日最低价变化曲线select goods_id, min(price) over(partition by goods_id,to_date(create_time) order by create_time) as price_minfrom goods-- max - 最大值 ...JDBC操作函数CREATE TEMPORARY FUNCTION dboutput AS &#39;org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput&#39;;select dboutput( &#39;jdbc:mysql://10.33.64.15:3306/report&#39;, &#39;admin&#39;, &#39;******&#39;, &#39;INSERT INTO QLujingAppDriverAndShipperDau( inputDate, driverAppDau, adAppDau ) VALUES (?,?,?)&#39;, input_date, driverapp_dau, adapp_dau) from increment.report_lujing_app_dau; GenericUDFDBOutput是hive自带函数,能在shell里直接使用，但是hue里不会加载这个jar,需要添加到AUX_JARS_PATH=[jar]或者在HIVE_HOME目录下新增一个auxlib目录，将该jar放到这个目录下,重启hive服务后，才能正常使用; 网上的在hue里配置修改辅助jar路径，可能导致让shell中Hive相关命令不能正常工作。" }, { "title": "Apache Flume 简单介绍", "url": "/posts/flume-introduce/", "categories": "Big data, Flume", "tags": "", "date": "2016-06-24 18:39:00 +0800", "snippet": "简介Flume是Cloudera公司研发的高可用、可靠、分布式日志采集系统。Flume由1~n个Agent组成，每个Agent部署到一个主机中，Agent由三大部件构成Flume组件结构图Flume系统有如下特点 由1 ~ n个Agent组成，每个Agent部署到一个主机中 一个Agent就是一个JVM进程，包含Source、Sink、Channel三组件，Source负责日志读取，Channel负责数据暂存，Sink负责数据写入 Source、Channel、Sink提供多种类型，不同类型组件可以通过编写的配置文件自由组合，例如Channel类型分为Memory、Disk等，Sink分为HDFS，HBase等 一个主机不需要部署多个Agent，一个Agent可以配置多个Source，对应不同的日志类型或者日志路径 Source收集到的日志数据被封装为Event，可能是日志记录，或者avro对象，由具体配置文件配置，后续Channel中暂存也是Events 不同类型或者路径的日志，即可以走各自不同的Channel，也可以合并到同一个Channel中去 一个Channel可以连接一个Sink，也可以连接多个不同Sink，甚至Sink可以连接到另外一个Source，当一个Sink连接到另外一个Source时，可以视为建立了一个多级日志流 三大组件均可以开发自定义类型，由于已经预留扩展接口和采用Java语言，实现起来较为容易Flume构成的多级日志流Channel组件是Flume架构设计特点，Fan-in、Fan-out得以实现，赋于整个系统灵活性。同时这种灵活性并非没有代价： Channel的设计在使用内存的情况下依据日志数据多少会消耗内存，换用磁盘类型Channel传输速度大幅下降 Event的设计为基于简单的扩展功能提供很好切入点，但同时也带来序列化反序列化的运算开销和内存占用增加基于以上优缺点，可知Flume满足以下场景 对日志传输实时性要求不高 (受暂存架构影响) 在使用日志数据时，不需要进行过于复杂的处理（本质只对单记录进行处理） 单主机日志量级不会过大（所有功能部件均在同一主机）" }, { "title": "Data Warehouse Base", "url": "/posts/data-warehouse-base/", "categories": "Data Warehouse", "tags": "updating", "date": "2016-02-18 00:00:00 +0800", "snippet": " OLTP：联机事务处理 OLAP：联机分析处理DatawarehouseODS：Operation Data Store数据贴源层，通常直接保存从外部导入的数据，不做任何处理DWD：Data Warehouse Detail数据明细层，对ODS层的数据进行清洗，过滤掉脏数据、如缺失关键值，或数据值明显错误等，形成可用的干净数据另一方面可以将ODS层的不同来源的数据按照规范统一后，使其具有一致的数据格式和规范，保存到同一处仓库也便于统一计算DM/DWM/DWB：Data Warehouse Basis数据基础层，也有称DM DWM（Data Warehouse Middle）保存轻度汇总数据，属于DWD向DWS过渡层次，提高数据复用度DWS：Data Warehouse Service以主题域建立的宽表，都是已经汇总好的数据，按照业务划分，生成字段比较多的宽表，提供后续业务查询，如OLAP分析和数据分发ADS：Application Data Service应用数据服务层，可直接用于应用的数据，通常放到Redis、ES等能满足快速响应的数据存储中，重点满足查询性能要求Data Model数据模型，将数据按照一定理论思想进行拆分、组合、使其形成一系列数据结构，可以更容易、高效满足最终工作目标并具备有很好的扩展性，这个设计过程可称之为数据建模。例如为满足业务事务的业务表建模，有三范式理论。为满足分析的数据仓库建模，有维度建模理论。 未完待续…" }, { "title": "Mysql Find Root Password", "url": "/posts/mysql-find-root-password/", "categories": "Database, MySQL", "tags": "", "date": "2016-02-05 00:00:00 +0800", "snippet": "个人开发机上的MySQL，因为长期不用root用户密码已经忘记了，现在需要创建一个新用户需要找回root用户密码进行操作macOS 系统下找回管理密码以mac操作系统为例，演示操作步骤 在’系统偏好设置’中关闭mysql服务 打开终端依次输入如下命令cd /usr/local/mysql/bin# 需要root权限执行相关操作sudo su./mysqld_safe --skip-grant-tables &amp;amp;./mysql进入MySQL命令行后输入如下MySQL命令flush privileges;set password for &#39;root&#39;@&#39;localhost&#39; = password(&#39;123456&#39;);exit;完成密码设置后测试一下, 用新密码登录MySQL，成功登入，接下来就可以正常模式启动mysql服务了./mysql -u root -p123456 这里密码设置为123456是方便开发机测试，在正式项目中不要使用这类简单密码" }, { "title": "抛弃IDE从命令行执行MySQL脚本", "url": "/posts/mysql-sql-scripts/", "categories": "Database, MySQL", "tags": "develop", "date": "2016-01-20 12:16:00 +0800", "snippet": "在命令行让MySQL执行已经编写好的SQL脚本，比如批量数据插入，或者更新等下面是基本信息 MySQL IP: 192.168.0.100 MySQL Port: 3306 数据库名: test 用户名：dev 脚本名：create-tables.sqlmysql -h192.168.0.100 -udev -p -Dtest&amp;lt;create-tables.sql 上面命令要求执行机上已经安装好MySQL客户端如果不希望安装MySQL客户端则需要登录mysql服务器scp create-tables.sql admin@192.168.0.100:~/ssh admin@192.168.0.100mysql -udev -p -Dtest&amp;lt;create-tables.sql 需要有MySQL服务器登录权限并且知道用户名和密码，一般正规公司环境有点难拿到权限" }, { "title": "开源镜像站收录", "url": "/posts/open-source-mirror/", "categories": "Mac, software", "tags": "develop, environment", "date": "2016-01-15 12:03:00 +0800", "snippet": "有一定时效性，仅供参考开源网站镜像：搜狐开源镜像站：http://mirrors.sohu.com/网易开源镜像站：http://mirrors.163.com/开源中国：http://mirrors.oschina.net/首都在线科技股份有限公司：http://mirrors.yun-idc.com/阿里云开源镜像：http://mirrors.aliyun.com/LUPA：http://mirror.lupaworld.com/常州贝特康姆软件技术有限公司(原cn99）：http://centos.bitcomm.cn/大学校园镜像：中山大学镜像：http://mirror.sysu.edu.cn/山东理工大学：http://mirrors.sdutlinux.org/哈尔滨工业大学：http://run.hit.edu.cn/中国地质大学：http://cugbteam.org/大连理工大学：http://mirror.dlut.edu.cn/西南林业大学 http://cs3.swfu.edu.cn/cs3guide.html北京化工大学（仅教育网可以访问），包含 CentOS 镜像：http://ubuntu.buct.edu.cn/天津大学：http://mirror.tju.edu.cn/西南大学：http://linux.swu.edu.cn/swudownload/Distributions/青岛大学：http://mirror.qdu.edu.cn/南京师范大学：http://mirrors.njnu.edu.cn/大连东软信息学院： http://mirrors.neusoft.edu.cn/浙江大学：http://mirrors.zju.edu.cn/兰州大学：http://mirror.lzu.edu.cn/厦门大学：http://mirrors.xmu.edu.cn/北京理工大学：http://mirror.bit.edu.cn (IPv4 only), http://mirror.bit6.edu.cn (IPv6 only)北京交通大学：http://mirror.bjtu.edu.cn (IPv4 only), http://mirror6.bjtu.edu.cn (IPv6 only), http://debian.bjtu.edu.cn (IPv4+IPv6)上海交通大学：http://ftp.sjtu.edu.cn/ (IPv4 only), http://ftp6.sjtu.edu.cn (IPv6 only)清华大学：http://mirrors.tuna.tsinghua.edu.cn/ (IPv4+IPv6), http://mirrors.6.tuna.tsinghua.edu.cn/ (IPv6 only), http://mirrors.4.tuna.tsinghua.edu.cn/ (IPv4 only)中国科学技术大学：http://mirrors.ustc.edu.cn/ (IPv4+IPv6), http://mirrors4.ustc.edu.cn/, http://mirrors6.ustc.edu.cn/东北大学：http://mirror.neu.edu.cn/ (IPv4 only), http://mirror.neu6.edu.cn/ (IPv6 only)华中科技大学：http://mirrors.hust.edu.cn/, http://mirrors.hustunique.com/电子科技大学：http://ubuntu.uestc.edu.cn/电子科大凝聚工作室(Raspbian单一系统镜像) http://raspbian.cnssuestc.org/电子科大星辰工作室(少数小众发布版镜像) http://mirrors.stuhome.net/PyPi 镜像豆瓣：http://pypi.douban.com/山东理工大学：http://pypi.sdutlinux.org/中山大学：http://mirror.sysu.edu.cn/pypi/V2EX：http://pypi.v2ex.com/simple/RubyGems 镜像中山大学：http://mirror.sysu.edu.cn/rubygems/山东理工大学：http://ruby.sdutlinux.org/淘宝网：http://ruby.taobao.org/npm 镜像cnpmjs：http://cnpmjs.org/" }, { "title": "Java Open Source Library", "url": "/posts/java-open-source-library/", "categories": "Java, Library", "tags": "updating", "date": "2016-01-10 11:58:00 +0800", "snippet": "记录一波开源库名称及用途介绍 包 名称 描述 com.sun.jersey jersey-* Restful框架 com.github.jsqlparser jsqlparser SQL词法分析 com.github.scopt scopt_2.11 SCALA命令行参数解析 com.yammer.metrics metrics-core 系统洞察库 io.dropwizard.metrics metrics-* 系统洞察库，上面这个库的升级版本3.x版本 net.jpountz.lz4 lz4 lz4数据压缩，速度最快 net.sf.jopt-simple jopt-simple java 命令行参数解析 net.sf.opencsv opencsv CSV文件读写 net.sf.py4j py4j python 与 java互相访问 net.sf.serfj serfj rest freamwork org.apache.commons commons-compress zip压缩与读取 org.apache.curator curator-* Curator是Netflix公司开源的一个Zookeeper客户端，与Zookeeper提供的原生客户端相比，Curator的抽象层次更高，简化了Zookeeper客户端的开发量。 org.apache.ivy ivy 依赖管理 org.aspectj aspectjweaver AOP依赖包 org.beanshell bsh-core 动态执行java代码 org.codehaus.janino janino 超级小但又超级快的Java编译器，被整合到apache commons jcl项目和JBoss Rules/Drools项目 org.bouncycastle bcprov-jdk15on 一种用于 Java 平台的开放源码的轻量级密码术包。它支持大量的密码术算法，并提供 JCE 1.2.1 的实现。因为 Bouncy Castle 被设计成轻量级的，所以从 J2SE 1.4 到 J2ME（包括 MIDP）平台，它都可以运行。它是在 MIDP 上运行的唯一完整的密码术包。 org.codehaus.jettison jettison json处理包 org.glassfish.hk2 hk2-* IoC/DI 容器框架 org.hamcrest hamcrest-* 用于测试的匹配器框架 org.objenesis objenesis 不使用构造方法创建Java对象 stax stax 解析XML xalan xalan XSLT转换器 未完待续…" }, { "title": "Spring 7种事务传播", "url": "/posts/spring-transaction/", "categories": "Java, Spring", "tags": "develop", "date": "2016-01-07 11:42:00 +0800", "snippet": " PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与 PROPAGATION_REQUIRED 类似的操作。" }, { "title": "mac使用技巧1", "url": "/posts/mac-tip1/", "categories": "Mac, tip", "tags": "", "date": "2015-12-23 20:55:00 +0800", "snippet": "在Finder中显示系统隐藏文件和文件夹defaults write com.apple.finder AppleShowAllFiles -boolean true ; killall Finder在Finder中恢复系统隐藏文件和文件夹defaults write com.apple.finder AppleShowAllFiles -boolean false ; killall Finder为普通非.开头的文件或文件夹加上隐藏属性，在Finder中不可见chflags hidden 文件名|目录名chflags nohidden 文件名|目录名OS X中的ftp 直接在命令行使用，打开终端输入ftp anonymous@ftp.mozilla.org，或者使用sftp通过ssh完成ftp的功能，例如sftp user@10.10.10.11。 使用第三方工具，比如FileZilla，用法和windows类似。 利用OS X原生ftp工具，从 Finder 菜单栏中进入“前往 - 连接服务器…”，输入 FTP 服务器地址（如：ftp://ftp.mozilla.org）点击地址栏右侧的 + 号按钮可以将当前地址加入“个人收藏服务器”点击“连接”按钮，按照提示进行身份验证成功后即可连接到 FTP 服务器。history打开终端输入history，所有的历史命令都会显示出来，想找某一条执行过的命令，还可以这样：history|grep apache找到左边的命令编号（例如是1001），在终端输入：!1001就可以执行原来那条命令了。Spotlight注释OS X的文件系统提供了Spotlight注释功能，可以帮助用户更有针对性的定位文件。选中一个文件或文件夹，command+I打开简介，在Spotlight注释功能中加入自己特定的关键词。关掉简介窗口，呼出Spotlight并输入刚才的关键词，可以准确定位到具备相关关键词注释的文件或文件夹。使用sips命令批量处理图片如果你想批量修改一批图片（尺寸、旋转、反转等），但是你有不会或没有PS，怎么办呢？使用sips命令可以高效完成这些功能，例如：# 把当前用户图片文件夹下的所有JPG图片宽度缩小为800px，高度按比例缩放sips -Z 800 ~/Pictures/*.JPG# 顺时针旋转90˚sips -r 90 ~/Pictures/*.JPG# 垂直反转sips -f vertical ~/Pictures/*.JPG更多命令可以用sips -h查看。快速便签选中要保留的文本，然后按下快捷键 Shift + Command + y多文件文件夹操作在windows中大家经常选中多个文件，右键-属性可以查看这些文件的大小。在Mac里同样的操作（选中多个文件，右键-显示简介），弹出的是各个文件或文件夹的简介，这让很多童靴困惑不解。其实我们只要在点右键的同时按住option键，显示简介就会变成显示检查器，点击显示检查器即可查看和操作批量文件。另外，我还经常用这种方式浏览图片，比如选中多张图片，option+右键，选中“幻灯片显示xx项”，就可以全屏浏览图片了。快捷键: Option + Command + i 和 Option + Command + y脚本等程序执行耗时查看比如你想知道在终端执行的某个程序耗时多久，对CPU等的使用情况，可以输入：time python fib.py输出结果：python fib.py 0.02s user 0.02s system 50% cpu 0.094 totalMac下的快速翻译OS X提供了三指轻拍查找的功能，什么意思呢？把光标移到一个单词上面，无需选中，三指轻拍，系统就会弹出词典显示相关单词的释义，非常方便。该功能可以在系统偏好设置-触控板里进行设置。" }, { "title": "The rename command", "url": "/posts/linux-rename-cmd/", "categories": "Linux", "tags": "command", "date": "2015-12-22 17:23:00 +0800", "snippet": "Linux 中 rename 命令可以对目录或文件进行改名，Sqoop会把已经完成传输的文件改名加上.COMPLETED后缀来区分是否需要处理，当我们做BUG复现调试时，需要再次传输这批文件，这时可以用文件改名去掉.COMPLETED后缀的方式实现，然而一个一个文件改名效率显然太低，rename命令可以支持批量改名将所有子目录中符合_2015-12-22_.log.COMPLETED名称的文件名中的.log.COMPLETED改为.log先看看目录结构及文件名规律$ tree ././├── task0001...│   ├── 12398374_2015-12-21_37652.log.COMPLETED│   ├── 12398374_2015-12-21_38273.log.COMPLETED│   ├── 13627779_2015-12-22_41826.log.COMPLETED│   ├── 13627778_2015-12-22_37652.log.COMPLETED│   ├── 18028345_2015-12-22_38273.log.COMPLETED│   ├── 18028345_2015-12-22_41826.log.COMPLETED...├── task0002...│   ├── 12398374_2015-12-21_37652.log.COMPLETED│   ├── 12398374_2015-12-21_38273.log.COMPLETED│   ├── 13627779_2015-12-22_41826.log.COMPLETED│   ├── 13627778_2015-12-22_37652.log.COMPLETED│   ├── 18028345_2015-12-22_38273.log.COMPLETED│   ├── 18028345_2015-12-22_41826.log.COMPLETED...可以看到同一天的文件分到不同文件夹，并且每个文件夹都有多天的数据，我只需要修改今天的数据让Sqoop重发今天的日志即可，因此命令行需要进行一些匹配过滤执行改名命令$ rename .log.COMPLETED .log */*_2015-12-22_*.log.COMPLETEDsuccessful!查看一下文件名是否更改正确$ tree ././├── task0001...│   ├── 12398374_2015-12-21_37652.log.COMPLETED│   ├── 12398374_2015-12-21_38273.log.COMPLETED│   ├── 13627779_2015-12-22_41826.log│   ├── 13627778_2015-12-22_37652.log│   ├── 18028345_2015-12-22_38273.log│   ├── 18028345_2015-12-22_41826.log...├── task0002...│   ├── 12398374_2015-12-21_37652.log.COMPLETED│   ├── 12398374_2015-12-21_38273.log.COMPLETED│   ├── 13627779_2015-12-22_41826.log│   ├── 13627778_2015-12-22_37652.log│   ├── 18028345_2015-12-22_38273.log│   ├── 18028345_2015-12-22_41826.log...执行成功！" }, { "title": "linux命令行模式常用快捷键", "url": "/posts/linux-edit-cmd/", "categories": "Linux", "tags": "command", "date": "2015-12-17 10:43:00 +0800", "snippet": "linux命令行模式常用快捷键Ctrl + u 剪切光标前的内容Ctrl + k 剪切光标到行末的内容Ctrl + v 粘贴Ctrl + e 移动光标到行末Ctrl + a 移动光标到行首Alt + f 跳向下一个空格Alt + b 跳回上一个空格Alt + backspace 删除前一个单词Ctrl + w&amp;lt;/kbc&amp;gt; 剪切光标前一个单词Shift + Insert 向终端内粘贴文本" }, { "title": "Hive 任务调优参数", "url": "/posts/hive-task-performance/", "categories": "Big data, Hive", "tags": "performance", "date": "2015-10-23 13:40:00 +0800", "snippet": "最近在做 Hive 集群搭建配置，记录对任务执行性能影响较大的参数#每个Map最大输入大小set mapred.max.split.size=256000000;#一个节点上split的至少的大小 set mapred.min.split.size.per.node=100000000; #一个交换机下split的至少的大小set mapred.min.split.size.per.rack=100000000; #执行Map前进行小文件合并set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;#在Map-only的任务结束时合并小文件set hive.merge.mapfiles = true; #在Map-Reduce的任务结束时合并小文件set hive.merge.mapredfiles = true; #合并文件的大小set hive.merge.size.per.task = 256000000; #当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件mergeset hive.merge.smallfiles.avgsize=16000000; " }, { "title": "修改mac启动台(dock)", "url": "/posts/mac-dock/", "categories": "Mac, tip", "tags": "", "date": "2015-10-21 18:55:00 +0800", "snippet": "mac dock排列是固定4行7列布局，刚开始使用没感觉有什么不妥，后来安装应用多了dock变成两页，很多时间需要在dock中滑动翻页才能找到所需应用，这时候就想着能不能控制dock页数在两页，第一页显示系统默认应用，第二页显示自己安装的应用。将多个应用合并到图标夹是一个办法，但是这样一来需要多一次点击才能访问应用，能不能通过修改每页排列的图标数来实现呢，经过一番查找，果然是有办法可以实现的。打开Terminal（终端），输入如下命令修改图标列数和图标行数修改图标列数，最后的的数字即为想要的列数，可以按需修改defaults write com.apple.dock springboard-columns -int 7修改图标行数，最后的数字即为行数，可以按需修改defaults write com.apple.dock springboard-rows -int 5重置图标排列顺序 (可选操作，会导致图标排列顺序及分类还原)defaults write com.apple.dock ResetLaunchPad -bool TRUE重启Dock生效killall Dock如果想要恢复系统默认设置，可以执行如下代码defaults write com.apple.dock springboard-columns Defaultdefaults write com.apple.dock springboard-rows Defaultdefaults write com.apple.dock ResetLaunchPad -bool TRUEkillall Dock" }, { "title": "SQL Joins", "url": "/posts/sql-join-img/", "categories": "Database, SQL Language", "tags": "develop, image", "date": "2015-09-17 21:09:00 +0800", "snippet": "SQL 中的 joins 关联，用一张图来说明白" }, { "title": "ERP Web2 BigData", "url": "/posts/erp-web2-bigdata/", "categories": "Big data", "tags": "develop, image", "date": "2015-09-17 21:09:00 +0800", "snippet": "ERP Web2 BigData 对比图" }, { "title": "mac系统特殊符号输入", "url": "/posts/mac-symbol/", "categories": "Mac, tip", "tags": "", "date": "2015-08-10 20:55:00 +0800", "snippet": " OS X下的特殊字符输入有很多快捷键 并且不用依赖第三方输入法 直接可用非常方便 说明 快捷键 字符 美元 Shift + 4 $ 美分 Option + 4 ¢ 英镑 Option + 3 £ 人民币 Option + y ¥ 欧元 Shift + Option + 2 € 波折号 Option + - 或 Shift + Option + - – — 省略号 Option + ; … 约等于 Option + x ≈ 度 Shift + Option + 8 • 除号 Option + / ÷ 无穷大 Option + 5 ∞ 小于等于 Option + , ≤ 大于等于 Option + . ≥ 不等于 Option + = ≠ 正负 Shift + Option + = ± 圆周率Pi Option + p π 平方根 Option + v √ 总和 Option + w ∑ 商标Trademark Option + 2 ™ 注册 Option + r ® 版权 Option + g © " }, { "title": "Hadoop Yarn Basis", "url": "/posts/hadoop-yarn-basis/", "categories": "Big data, Hadoop", "tags": "architecture", "date": "2015-07-21 00:00:00 +0800", "snippet": "Hadoop 包含三大组件分别是：HDFS（分布式文件系统）、MR（分布式计算引擎）、YARN（分布式资源管理）YARN to be continue…" }, { "title": "Hadoop MR Basis", "url": "/posts/hadoop-mr-basis/", "categories": "Big data, Hadoop", "tags": "", "date": "2015-07-15 00:00:00 +0800", "snippet": "Hadoop 包含三大组件分别是：HDFS（分布式文件系统）、MR（分布式计算引擎）、YARN（分布式资源管理）MR (MapReduce)MR是Hadoop中的计算模型，主要由三个阶段组成：Map &amp;gt; shuffle &amp;gt; Reduce。Map阶段是对数据进行拆分(split)，将数据转化为键值对；Reduce是合并，将具有相同的Key的Value进行聚合，最终输出全部聚合之后的结果。shuffle阶段包含Map shuffle和Reduce shuffle。Map shuffle即是在Map端的shuffle，Reduce shffle即是在Reduce端的shuffle，我们在编写MR程序时，只需要编写Map逻辑代码和Reduce逻辑代码，shuffle是由系统自动实现。Map shuffle 对 Map拆分好的键值对进行分区，将同一分区的数据进行聚合、排序，得到的结果按照一个分区一个文件写入磁盘。Reduce shuffle 读取 Map shuffle写好的分区文件，将多个分区文件按照相同 key 值进行聚合，形成一个个键值对，所不同的是这次的值其实是多个值组成的数组。这些结果将做为Reduce阶段的输入参数进行处理。" }, { "title": "Hadoop HDFS Basis", "url": "/posts/hadoop-hdfs-basis/", "categories": "Big data, Hadoop", "tags": "architecture", "date": "2015-07-11 00:00:00 +0800", "snippet": "Hadoop 包含三大组件分别是：HDFS（分布式文件系统）、MR（分布式计算引擎）、YARN（分布式资源管理）HDFSHDFS（Hadoop Distributed File System），名为Hadoop分布式文件系统，具有高容错、高吞吐量、容易扩展、高可靠性 的特征，是Hadoop生态重要的存储系统。HDFS是主从结构的分布式系统，一个典型的HDFS系统包含一个Namenode和几个Datanode组成，用户通过Client和相应的配置同Namenode和Datanode交互访问文件系统.HDFS以数据块为基本存储单位，一个数据块的默认大小ver1.x为64m，ver2.x为128m，ver3.x为256m，写入HDFS的文件在写满一个数据块后再写下一个数据块，NamenodeNamenode保存命名空间信息，包含了系统的文件目录树、文件/目录信息以及文件的数据块索引。同时还保存有数据块与数据节点的对应关系，这部分使用时常驻内存中，由启动时动态构建，为避免关机断电后的数据丢失在磁盘上肯定也有存储，这就是两个重要文件fsimage及editlog。Namenode支持HA，此时共两个节点，一个是Active Namenode，一个是Standby Namenode。Active Namenode负责处理HDFS系统的所有客户端请求，读写响应、与Datanode通讯等。Standby Namenode会实时同步(通过JournalNode)Active Namenode的命名空间数据，保持与Standby Namenode一致，一但发现 Active Namenode出现故障，Standby Namenode会升级成Active Namenode保障集群功能正常运行。fsimage &amp;amp; editlogfsimage保存截止某个时刻的全量命名空间的数据信息，而editlog保存此时刻之后一系列变更和修改，将两者合并就可以得到截止当前时刻的命名空间最终状态。fsimage相当于全量数据，保存的是最终状态，editlog相当于是流水数据保存了过程状态，可知editlog文件大小是随着系统使用时间不断增长的，为了避免文件过大必须在一个周期后将editlog里的记录与fsimage进行合并，新成一个新的fsimage文件。在非HA架构中fsimage和editlog文件的合并由Namenode完成，在HA架构中合并操作可由备份节点完成，备份节点在运行时不断同步editlog文件记录(JournalNode机制)，在合适的时间进行fsimage和editlog文件的合并，然后将合并后的fsimage同步给活动节点，活动节点检验后替换旧的fsimage文件并进行加载，活动节点的工作更轻松、系统运行更顺畅了。JournalNodeJournalNode是在集群中启动的逻辑节点，通常是在某些Datanode上启动的进程，JournalNode目的是为了让Standby Namenode与Active Namenode保持同步，Active Namenode对命名空间所做的修改会持久化到editlog文件当中，而文件必须要同步到N-(N-1)/2个JournalNode节点才能保证安全性，换言之集群有3个JournalNode最多充许1个挂掉，有5个JournalNode最多充许2个挂掉，一但Active Namenode出现故障，Standby Namenode可以从JournalNode中读取全部editlog，切换到Active，保障安全的故障转移。DatanodeDatanode主要功能是负责文件读写，会按照一定间隔向Namenode发送心跳、数据块汇报以及缓存汇报；Namenode则会响应心跳包，响应中有可能带有创建文件、删除文件或者复制文件的命令。写文件简略流程 调用Client的API创建文件准备开始写文件 Clinet向Namenode发送请求创建文件 Namenode首先对请求进行合法性验证 Namenode根据最新的Datanode状态信息（网络延时、负载、可用磁盘空间）等信息，给Client返回创建的文件ID及所在节点、数据块索引等信息 client通过返回信息建立与Datanode的通讯 成功后Clinet与Datanode建立流式通道并开始写入文件数据 写入完成后Client关闭与Datanode的网络链接 Datanode按照复制策略将刚刚写入的文件复制到另外两个节点（默认2副本）读文件简略流程 调用Client的APi准备开始读文件 Client向Namenode发送请求读取文件(HadoopRPC) Namenode查询命令空间，如果没有找到文件则返回异常，如果找到文件则根据 文件 &amp;gt; 数据块 &amp;gt; Datanode 的索引将数据返回 Client 与 Datanode 进行网络通讯 网络通讯成功后，Client 与 Datanode 建立流式通道读取文件数据 完成后关闭 Client 与 Datanode 的流式通道 Datanode 将本次访问情况记录并上报Namenode高容错性Namenode支持HA降低单点故障风险，文件3副本机制保障不会由于个别Datanode节点故障导致文件丢失高吞吐量文件读写IO在Client与Datanode之间进行，由于Datanode通常有多个可以均摊负载，一个文件分为多个数据块也可以支持并行读取，文件数据读写基于流式接口执行有利于批量数据处理同时提高了吞吐量容易扩展Datanode支持横向扩展，由此增加存储能力。Namenode可以通过切换到Sandby的方式调整节点硬件配置。高可靠性Namenode 命名空间数据都写入磁盘文件fsimage，存储全量的命名空间数据，editlog保存命名空间修改日志，每间隔一段时间对两个文件进行合并，产生新的fsimage文件。JournalNode分布式多进程同步Active Namenode的editlog日志，在故障转移过程中由JournalNode保障editlog的完全同步，保障Namenode的故障转移。Datanode中存储的文件默认存储另外2个副本到其它Datanode，保障文件数据不会由个别Datanode故障而丢失。。租约管理HDFS文件是write-once-read-many，因此不支持客户端的并行写操作，那么这里就需要一种机制保证对HDFS文件的互斥操作。HDFS使用租约（lease）机制来实现这个功能，租约就是HDFS系统给Client一段时间内对某文件的独占锁定，客户端在完成写入操作关闭文件时即释放租约，如果在租约期限内未完成操作，则需要进行续约，否则会由HDFS收回租约从而写入失败。 租约有软限制（softLimit）和硬限制（hardLimit），软限制为默认60秒可以在集群配置中进行配置，硬限制为60分钟无法更改" }, { "title": "MySQL数据备份命令mysqldump", "url": "/posts/mysqldump-cmd/", "categories": "Database, MySQL", "tags": "command", "date": "2015-06-11 14:48:00 +0800", "snippet": "MySQL数据库提供mysqldump命令进行数据备份# 备份MySQL数据库的命令mysqldump -hhostname -uusername -ppassword databasename &amp;gt; backupfile.sql# 备份MySQL数据库为带删除表的格式，能够让该备份覆盖已有数据库而不需要手动删除原有数据库。mysqldump -–add-drop-table -uusername -ppassword databasename &amp;gt; backupfile.sql# 直接将MySQL数据库压缩备份mysqldump -hhostname -uusername -ppassword databasename | gzip &amp;gt; backupfile.sql.gz# 备份MySQL数据库某个(些)表mysqldump -hhostname -uusername -ppassword databasename specific_table1 specific_table2 &amp;gt; backupfile.sql# 同时备份多个MySQL数据库mysqldump -hhostname -uusername -ppassword –databases databasename1 databasename2 databasename3 &amp;gt; multibackupfile.sql# 仅仅备份数据库结构mysqldump –no-data –databases databasename1 databasename2 databasename3 &amp;gt; structurebackupfile.sql# 备份服务器上所有数据库mysqldump –all-databases &amp;gt; allbackupfile.sql# 还原MySQL数据库的命令mysql -hhostname -uusername -ppassword databasename &amp;lt; backupfile.sql# 还原压缩的MySQL数据库gunzip &amp;lt; backupfile.sql.gz | mysql -uusername -ppassword databasename# 将数据库转移到新服务器mysqldump -uusername -ppassword databasename | mysql –host=*.*.*.* -C databasename" }, { "title": "完全卸载Inteliij IDEA", "url": "/posts/idea-uninstall/", "categories": "Mac, software", "tags": "idea", "date": "2015-06-09 13:22:00 +0800", "snippet": "在mac系统中安装了idea之后，因为升级更换版本等原因需要卸载重装，但是如果没有卸载干净容易出现一些奇怪问题mac系统标准方式卸载就是在dock中按着应用图标不动，3秒后图标左上会出现一个 “X” 号，点击它就可以删除软件，但有时候这个 “X” 不会出现，这时就要祭出 terminal大法 了打开终端，输入如下命令cd /Applicationssudo rm -rf Inteliij\\ IDEA.app当应用名称包含空格时，命令中需要在空格高加上 \\ 进行转义处理应用主体删除后，再删除应用运行时留下的配置、日志、插件等其它遗留目录，目录用途和路径说明如下： Config: ~/Library/Preferences/IdeaIC13 System: ~/Library/Caches/IdeaIC13 Plugins: ~/Library/Application Support/IdeaIC13 Logs: ~/Library/Logs/IdeaIC13一口气全部删除rm -rf \\ ~/Library/Preferences/IdeaIC13\\ ~/Library/Caches/IdeaIC13\\ ~/Library/Application\\ Support/IdeaIC13\\ ~/Library/Logs/IdeaIC13" }, { "title": "vi vim中的文本替换", "url": "/posts/linux-vi-replace/", "categories": "Linux, vi/vim", "tags": "", "date": "2015-06-09 13:22:00 +0800", "snippet": "vi/vim 是Linux下文件编辑软件，其中文本替换功能经常用到，如果熟悉常用替换命令，工作效率大幅提升# 替换当前行第一个 vivian 为 sky ：s/vivian/sky/ # 替换当前行所有 vivian 为 sky ：s/vivian/sky/g # 替换第 n 行开始到最后一行中每一行的第一个 vivian 为 sky ：n，$s/vivian/sky/ # 替换第 n 行开始到最后一行中每一行所有 vivian 为 sky# n 为数字，若 n 为 .，表示从当前行开始到最后一行 ：n，$s/vivian/sky/g # 替换每一行的第一个 vivian 为 sky：%s/vivian/sky/（等同于 ：g/vivian/s//sky/） #（等同于 ：g/vivian/s//sky/g） 替换每一行中所有 vivian 为 sky# 可以使用 `#` 作为分隔符，此时中间出现的 / 不会作为分隔符：%s/vivian/sky/g# 替换当前行第一个 vivian/ 为 sky/：s#vivian/#sky/# #（使用+ 来 替换 / ）： /oradata/apras/替换成/user01/：%s+/oradata/apras/+/user01/apras1+apras1/ 本篇中的替换语法同样适用于sed命令工具" }, { "title": "从Windows复制到mac的文本文件无法打开", "url": "/posts/mac-iconv-cmd/", "categories": "Mac, tip", "tags": "command", "date": "2015-05-28 11:02:00 +0800", "snippet": "从原来的Windows系统迁移过来一些文本文件，发现在mac上打不开了，但在Windows上能正常打开，后来查到原因是文件的编码问题，在简体中文环境下Windows记事本创建的文本文件默认为GBK编码，mac上默认打开编码则为UTF-8，由于编码不正确mac会直接提示无法打开。解决办法有两种： 在mac上不要直接双击文件打开，而是先在dock中或者finder的应用程序中打开文本编辑器，在应用弹出的文件窗口中选中要打开的文件，并且点击下方的选项按钮并在纯文本编码:一栏选择中文(GB 18030) 使用mac系统自带命令iconv将文件转码为UTF-8编码，以后就可以直接打开该文件了iconv 转换命令iconv -f GBK -t UTF-8 gbk.txt &amp;gt; utf8.txt-f参数后是原文件编码，-t参数后是转换后的编码，执行后得到的就是可以直接正常打开的文本文件了想知道iconv命令支持哪些编码格式，可以执行下面命令iconv -l 本文针对Windows 7及更早版本的Windows系统，Windows 10及其之后版本修改了默认存储编码为UTF-8，结束了一直以来的编码问题" }, { "title": "Mysql Initializing", "url": "/posts/MySQL-initializing/", "categories": "Database, MySQL", "tags": "", "date": "2015-05-27 00:00:00 +0800", "snippet": "MySQL 数据库安装好以后，使用前需要对数据库初始化，比如创建库表、创建用户、授权等，下面写一个典型建库、建表、建用户的流程创建新的数据库并授权-- 创建数据库并指定字符集utf8mb4create database if not exists ohemin default charset utf8mb4 collate utf8mb4_general_ci;-- 创建数据库用户并指定密码create user &#39;devuser&#39;@&#39;localhost&#39; identified by password(&#39;123456&#39;);-- 查询用户select user,host from mysql.user;-- 删除用户drop user devuser@localhost;drop user devuser@&#39;%&#39;;-- 更改密码方法1, 密码实时更新;set password for test = password(&#39;123456&#39;);-- 更改密码方法2, 需要刷新;update mysql.user set password=password(&#39;123456&#39;) where user=&#39;test&#39;;flush privileges; 建库字符集可以选择utf8或者utf8mb4，它们的区别在于utf8mb4可以在记录值中正确支持操作系统或者社交软件的表情符号，而utf8则会出现错误。好消息是高版本的MySQL已经将这两个字符集合并，无论选择utf8或者utf8mb4都可以正常使用表情符号. collate utf8mb4_general_ci 是数据库校对规则，ci是case insensitive缩写，表示大小不敏感，在使用上建表语句或者查询中的大写表名或字段名均转为小写；如果想使用基于Oracle数据库区分大小写的规则可用 collate utf8mb4_general_cs，cs即case sensitive缩写，表示大小写敏感。用户分配权限-- 授予用户devuser通过外网IP对数据库&#39;os&#39;的全部权限grant all privileges on os.* to &#39;devuser&#39;@&#39;%&#39; identified by &#39;devuser123&#39;;-- 授予用户devuser通过任意P对于数据库os中表的创建\\修改\\删除权限 ,以及表数据的增删改查权限 grant create,alter,drop,select,insert,update,delete on happyos.* to happyos@&#39;%&#39;;-- 刷新权限flush privileges;" }, { "title": "mac中安装MySQL数据库", "url": "/posts/mac-mysql-install/", "categories": "Database, MySQL", "tags": "develop, environment", "date": "2015-05-26 19:22:00 +0800", "snippet": "apache 已经配置好并启动了，下一步要安装Java开发必备数据库，MySQL这次安装比较简单，先到官网下载安装包： 浏览器打开 https://dev.mysql.com/downloads/ 在打开的页面中点击 MySQL Community Server，这个是社区版，开发者免费使用 进入新页面后会有系统版本等选择，我们选择自己对应操作系统的版本，然后点击Download按钮开始下载 下载完成后文件类似mysql-5.7.23-xxx.dmg，点击打开，在弹出的窗口中仁显示一个名称相似但文件后缀为pkg的文件，继续双击打开，会启动一个安装程序，一路next完成安装 安装完成后，会在系统偏好设置中，新增加了一个名为MySQL的图标，点击进去后可以看到一个Start MySQL Server按钮，就是这里启动MySQL了，下方还有一个复选框，可以设置是否在系统启动时自动运行MySQL服务，可以视情况自己决定是否要勾选。至此开发环境MySQL也已安装完成" }, { "title": "使用mac自带apache服务", "url": "/posts/mac-apache-install/", "categories": "Mac, apache", "tags": "develop, environment", "date": "2015-05-25 09:22:00 +0800", "snippet": "mac 系统通常默认自带有Apache http服务，我们要做的就是把它启动管理apache服务# 启动服务sudo apachectl start# 停止服务sudo apachectl stop# 重启服务sudo apachectl restart管理mac启动服务# apache服务随系统启动launchctl load -w /System/Library/LaunchDaemons/org.apache.httpd.plist# 验证下是不是设置上了launchctl list | grep httpd# 后悔了，不想让服务自动启动launchctl unload -w /System/Library/LaunchDaemons/org.apache.httpd.plist# 查看所有的启动服务launchctl list# 禁用一个启动服务launchctl disable /System/Library/LaunchDaemons/org.apache.httpd.plist# 启用一个启动服务launchctl enable /System/Library/LaunchDaemons/org.apache.httpd.plist# 杀死启动服务，用于解决一些停止响应的服务launchctl disable /System/Library/LaunchDaemons/org.apache.httpd.plist# 直接启动launchctl start /System/Library/LaunchDaemons/org.apache.httpd.plist# 直接停止launchctl stop /System/Library/LaunchDaemons/org.apache.httpd.plist编辑apache配置vi /etc/apache2/httpd.conf建将DocumentRoot目录改掉# 默认网页文件在 /Library/WebServer/Documents 管理起来不方便，改为更方便管理的目录# Mac版命令，第1个参数为空字符串，意思是原地替换，如果不为空则替换前产生一处备份文件，将参数中字符添加到备份文件扩展名后sudo sed -i &#39;&#39; &#39;s/\\/Library\\/WebServer\\/Documents/\\/Users\\/[your name]\\/www/g&#39; &#39;/etc/apache2/httpd.conf&#39;# Linux版命令sudo sed -i &#39;s/\\/Library\\/WebServer\\/Documents/\\/Users\\/[your name]\\/www/g&#39; &#39;/etc/apache2/httpd.conf&#39;" }, { "title": "开篇", "url": "/posts/started/", "categories": "", "tags": "", "date": "2015-05-23 20:55:00 +0800", "snippet": "现在才开始维护博客不觉太晚吗？从事软件开发多年，才开始维护自己的技术博客，似乎有些晚。回想当初为什么没有维护在线博客，自己给出的理由有如下两点: 个人习惯及对博客网站的不信任 由于个人入行时互联网刚刚兴起，网络资料及上网条件还不似今天这般发达，习惯于在个人PC上维护日记、文档等。在线博客网站刚刚兴起，博客网站如雨后春笋、层出不穷。但我感觉盈利模式不明，从长远看经营情况不稳定，什么时候关闭网站谁也说不准，而且一但网站关闭文章内容丢失损失巨大。 选择困难 选择哪种方式开始博客，第三方？新浪博客、CSDN、简书、QQ空间，优点是使用方便，完成账号注册后就可以发表自己的第一篇文章了，就是省事。不过从此就绑定在该平台上，后续如果运营方修改政策如：内容限制、服务收费、甚至关闭服务，对用户来说用得越久损失越大；自建博客？动态站点 or 静态站点、还是基于模板生成的伪静态，各有优缺点，当时动态博客主流方案是买主机，部署一套PHP博客站点，从界面到功能再到维护，其实搭建管理成本并不低；静态网站直接编写HTML文件创建成本足够低，但失去扩展性，后续要加个功能改个外观啥的就很难了。想来想去没有满意的方案就暂时搁置了，反正写文件存在自己电脑上也没啥问题。 那以前这些疑虑解决了吗？博客站经过十多年的发展，市场大浪淘沙，剩下来的服务商都有自己的生存之道，突然关闭的风险小了很多。自建博客的方案也已经成熟，主流的几种面向有不同需求的人群。无技术背景选择博客网站或者云服务商的博客网站+云服务器套餐，淘宝上也有大把各式博客建站，完全可以极低价格搭建一套个人博客站。懂点技术的一个比较好的选择使用GitHub Pages建站，除了花点时间无其它成本，而好处是上面两种方式无法比拟的，我随便列举几个：无域名服务器费用、本地和和远程双副本数据有保障、方便迁移和多站同步…总结就是:真香！" } ]
